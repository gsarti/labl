{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>labl</code> \ud83c\udff7\ufe0f","text":""},{"location":"api/data/dataset/","title":"Dataset","text":""},{"location":"api/data/dataset/#labl.data.base_sequence.BaseLabeledDataset","title":"labl.data.base_sequence.BaseLabeledDataset","text":"<pre><code>BaseLabeledDataset(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseLabeledSequence[EntryType | BaseMultiLabelEntry[EntryType]]</code>, <code>ABC</code></p> <p>Base class for all dataset classes containing <code>BaseLabeledEntry</code> objects.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/dataset/#labl.data.base_sequence.BaseLabeledDataset.get_agreement","title":"get_agreement","text":"<pre><code>get_agreement(\n    other: BaseLabeledSequence[\n        EntryType | BaseMultiLabelEntry[EntryType]\n    ]\n    | None = None,\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput\n</code></pre> <p>Compute the inter-annotator agreement for the token labels of all label sets using Krippendorff's alpha.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def get_agreement(\n    self,\n    other: BaseLabeledSequence[EntryType | BaseMultiLabelEntry[EntryType]] | None = None,\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput:\n    \"\"\"Compute the inter-annotator agreement for the token labels of all label sets using\n    [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha).\n    \"\"\"\n    self._validate_single_label_type()\n    if other is not None:\n        other._validate_single_label_type()\n        if self.label_types[0] != other.label_types[0]:\n            raise RuntimeError(\n                f\"Label type does not match: {self.label_types[0]} vs {other.label_types[0]}.\\n\"\n                \"Transform the annotations using `.relabel` to ensure a single type is present.\"\n            )\n    labels_array = self._get_labels_array(other=other, dtype=self.label_types[0])\n    return get_labels_agreement(\n        label_type=self.label_types[0],\n        labels_array=labels_array,\n        level_of_measurement=level_of_measurement,\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset","title":"labl.data.labeled_dataset.LabeledDataset","text":"<pre><code>LabeledDataset(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseLabeledDataset[LabeledEntry]</code></p> <p>Dataset class for handling collections of <code>LabeledEntry</code> objects.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>list[LabeledEntry]</code> <p>A list of LabeledEntry objects.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans","title":"from_spans  <code>classmethod</code>","text":"<pre><code>from_spans(\n    texts: list[str],\n    spans: list[list[Span]] | list[list[SpanType]],\n    infos: list[InfoDictType] | None = None,\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n) -&gt; LabeledDataset\n</code></pre> <p>Create a <code>LabeledDataset</code> from a set of texts and one or more spans for each text.</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>The set of text.</p> required <code>list[list[Span]] | list[list[dict[str, str | int | float | None]]]</code> <p>A list of spans for each text.</p> required <code>list[dict[str, str | int | float | bool]] | None</code> <p>A list of dictionaries containing additional information for each entry. If None, no additional information is added. Defaults to None.</p> <code>None</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> Source code in <code>labl/data/labeled_dataset.py</code> <pre><code>@classmethod\ndef from_spans(\n    cls,\n    texts: list[str],\n    spans: list[list[Span]] | list[list[SpanType]],\n    infos: list[InfoDictType] | None = None,\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n) -&gt; \"LabeledDataset\":\n    \"\"\"Create a `LabeledDataset` from a set of texts and one or more spans for each text.\n\n    Args:\n        texts (list[str]):\n            The set of text.\n        spans (list[list[Span]] | list[list[dict[str, str | int | float | None]]]):\n            A list of spans for each text.\n        infos (list[dict[str, str | int | float | bool]] | None):\n            A list of dictionaries containing additional information for each entry.\n            If None, no additional information is added. Defaults to None.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    if infos is None:\n        infos = [{}] * len(texts)\n    return cls(\n        [\n            LabeledEntry.from_spans(\n                text,\n                span,\n                tokenizer=tokenizer,\n                info=info,\n            )\n            for text, span, info in tqdm(\n                zip(texts, spans, infos, strict=True),\n                desc=\"Creating labeled dataset\",\n                total=len(texts),\n                unit=\"entries\",\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans(texts)","title":"<code>texts</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans(spans)","title":"<code>spans</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans(infos)","title":"<code>infos</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_spans(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged","title":"from_tagged  <code>classmethod</code>","text":"<pre><code>from_tagged(\n    tagged: list[str],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n    tokenizer_kwargs: dict = {},\n    infos: list[InfoDictType] | None = None,\n) -&gt; LabeledDataset\n</code></pre> <p>Create a <code>LabeledDataset</code> from a set of tagged texts.</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>The set of tagged text.</p> required <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>list[dict[str, str | int | float | bool]] | None</code> <p>A list of dictionaries containing additional information for each entry. If None, no additional information is added. Defaults to None.</p> <code>None</code> <code>list[str]</code> <p>A list of tags to keep.</p> <code>[]</code> <code>list[str]</code> <p>A list of tags to ignore.</p> <code>[]</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> Source code in <code>labl/data/labeled_dataset.py</code> <pre><code>@classmethod\ndef from_tagged(\n    cls,\n    tagged: list[str],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n    tokenizer_kwargs: dict = {},\n    infos: list[InfoDictType] | None = None,\n) -&gt; \"LabeledDataset\":\n    \"\"\"Create a `LabeledDataset` from a set of tagged texts.\n\n    Args:\n        tagged (list[str]):\n            The set of tagged text.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        infos (list[dict[str, str | int | float | bool]] | None):\n            A list of dictionaries containing additional information for each entry.\n            If None, no additional information is added. Defaults to None.\n        keep_tags (list[str]): A list of tags to keep.\n        ignore_tags (list[str]): A list of tags to ignore.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    if infos is None:\n        infos = [{}] * len(tagged)\n    return cls(\n        [\n            LabeledEntry.from_tagged(\n                text,\n                tokenizer=tokenizer,\n                keep_tags=keep_tags,\n                ignore_tags=ignore_tags,\n                info=info,\n            )\n            for text, info in tqdm(\n                zip(tagged, infos, strict=True), desc=\"Creating labeled dataset\", total=len(tagged), unit=\"entries\"\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(tagged)","title":"<code>tagged</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(infos)","title":"<code>infos</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(keep_tags)","title":"<code>keep_tags</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(ignore_tags)","title":"<code>ignore_tags</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tagged(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens","title":"from_tokens  <code>classmethod</code>","text":"<pre><code>from_tokens(\n    tokens: list[list[str]],\n    labels: Sequence[Sequence[LabelType]],\n    infos: list[InfoDictType] | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n) -&gt; LabeledDataset\n</code></pre> <p>Create a <code>LabeledDataset</code> from a set of tokenized texts.</p> <p>Parameters:</p> Name Type Description Default <code>list[list[str]] | None</code> <p>A list of lists of string tokens.</p> required <code>list[list[str | int | float | None]] | None</code> <p>A list of lists of labels for the tokens.</p> required <code>list[dict[str, str | int | float | bool]] | None</code> <p>A list of dictionaries containing additional information for each entry. If None, no additional information is added. Defaults to None.</p> <code>None</code> <code>list[str]</code> <p>A list of labels to keep.</p> <code>[]</code> <code>list[str]</code> <p>A list of labels to ignore.</p> <code>[]</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> Source code in <code>labl/data/labeled_dataset.py</code> <pre><code>@classmethod\ndef from_tokens(\n    cls,\n    tokens: list[list[str]],\n    labels: Sequence[Sequence[LabelType]],\n    infos: list[InfoDictType] | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n) -&gt; \"LabeledDataset\":\n    \"\"\"Create a `LabeledDataset` from a set of tokenized texts.\n\n    Args:\n        tokens (list[list[str]] | None):\n            A list of lists of string tokens.\n        labels (list[list[str | int | float | None]] | None):\n            A list of lists of labels for the tokens.\n        infos (list[dict[str, str | int | float | bool]] | None):\n            A list of dictionaries containing additional information for each entry.\n            If None, no additional information is added. Defaults to None.\n        keep_labels (list[str]): A list of labels to keep.\n        ignore_labels (list[str]): A list of labels to ignore.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    if infos is None:\n        infos = [{}] * len(tokens)\n    return cls(\n        [\n            LabeledEntry.from_tokens(\n                tokens=tokens[idx],\n                labels=labels[idx],\n                keep_labels=keep_labels,\n                ignore_labels=ignore_labels,\n                tokenizer=tokenizer,\n                info=infos[idx],\n            )\n            for idx in tqdm(range(len(tokens)), desc=\"Creating LabeledDataset\", total=len(tokens), unit=\"entries\")\n        ]\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(labels)","title":"<code>labels</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(infos)","title":"<code>infos</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(keep_labels)","title":"<code>keep_labels</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(ignore_labels)","title":"<code>ignore_labels</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/dataset/#labl.data.labeled_dataset.LabeledDataset.from_tokens(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset","title":"labl.data.edited_dataset.EditedDataset","text":"<pre><code>EditedDataset(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseLabeledDataset[EditedEntry]</code></p> <p>Dataset class for handling collections of <code>EditedEntry</code> and <code>MultiEditEntry</code> objects.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>list[EditedEntry] | list[MultiEditEntry]</code> <p>A list of <code>EditedEntry</code> or <code>MultiEditEntry</code> objects.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits","title":"from_edits  <code>classmethod</code>","text":"<pre><code>from_edits(\n    texts: list[str],\n    edits: list[str] | list[list[str]],\n    infos: list[InfoDictType]\n    | list[list[InfoDictType]]\n    | None = None,\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; EditedDataset\n</code></pre> <p>Create an <code>EditedDataset</code> from a set of texts and one or more edits for each text.</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>The set of text.</p> required <code>list[str] | list[list[str]] | None</code> <p>One or more edited version for each text.</p> required <code>list[dict[str, str | int | float | bool]] | list[list[dict[str, str | int | float | bool]]] | None</code> <p>A list of dictionaries containing additional information for each entry. If multiple edits are provided for each text, <code>infos</code> can be a list of lists of dictionaries (one per edit per entry). If None, no additional information is added. Defaults to None.</p> <code>None</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>bool</code> <p>Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of insertions and deletions in the original/edited texts, respectively. If false, those are merged to the next token to the right. Default: True.</p> <code>True</code> <code>str</code> <p>The label for substitutions. Default: \"S\".</p> <code>'S'</code> <code>str</code> <p>The label for insertions. Default: \"I\".</p> <code>'I'</code> <code>str</code> <p>The label for deletions. Default: \"D\".</p> <code>'D'</code> <code>str</code> <p>The token to use for gaps. Default: \"\u2581\".</p> <code>'\u2581'</code> Source code in <code>labl/data/edited_dataset.py</code> <pre><code>@classmethod\ndef from_edits(\n    cls,\n    texts: list[str],\n    edits: list[str] | list[list[str]],\n    infos: list[InfoDictType] | list[list[InfoDictType]] | None = None,\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; \"EditedDataset\":\n    \"\"\"Create an `EditedDataset` from a set of texts and one or more edits for each text.\n\n    Args:\n        texts (list[str]):\n            The set of text.\n        edits (list[str] | list[list[str]] | None):\n            One or more edited version for each text.\n        infos (list[dict[str, str | int | float | bool]] | list[list[dict[str, str | int | float | bool]]] | None):\n            A list of dictionaries containing additional information for each entry.\n            If multiple edits are provided for each text, `infos` can be a list of lists of dictionaries (one per\n            edit per entry). If None, no additional information is added. Defaults to None.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n        with_gaps (bool): Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of\n            insertions and deletions in the original/edited texts, respectively. If false, those are merged to the\n            next token to the right. Default: True.\n        sub_label (str): The label for substitutions. Default: \"S\".\n        ins_label (str): The label for insertions. Default: \"I\".\n        del_label (str): The label for deletions. Default: \"D\".\n        gap_token (str): The token to use for gaps. Default: \"\u2581\".\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    if infos is None:\n        infos = [{}] * len(texts)\n    return cls(\n        [\n            EditedEntry.from_edits(\n                text=text,\n                edits=edit,\n                tokenizer=tokenizer,\n                with_gaps=with_gaps,\n                sub_label=sub_label,\n                ins_label=ins_label,\n                del_label=del_label,\n                gap_token=gap_token,\n                info=info,\n            )\n            for text, edit, info in tqdm(\n                zip(texts, edits, infos, strict=True),\n                desc=\"Creating EditedDataset\",\n                total=len(texts),\n                unit=\"entries\",\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(texts)","title":"<code>texts</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(edits)","title":"<code>edits</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(infos)","title":"<code>infos</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(with_gaps)","title":"<code>with_gaps</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(sub_label)","title":"<code>sub_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(ins_label)","title":"<code>ins_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(del_label)","title":"<code>del_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe","title":"from_edits_dataframe  <code>classmethod</code>","text":"<pre><code>from_edits_dataframe(\n    df,\n    text_column: str,\n    edit_column: str,\n    entry_ids: str | list[str],\n    infos_columns: list[str] = [],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; EditedDataset\n</code></pre> <p>Create an <code>EditedDataset</code> from a <code>pandas.DataFrame</code> with edits.</p> <p>Every row in the DataFrame is an entry identified univocally by <code>entry_ids</code>. The <code>text_column</code> contains the original text, and the <code>edit_column</code> contains the edits. If multiple columns with the same <code>entry_ids</code> are present, they are all treated as edits of the same text.</p> <p>Parameters:</p> Name Type Description Default <code>DataFrame</code> <p>The DataFrame containing the text and edits.</p> required <code>str</code> <p>The name of the column in the dataframe containing the original text.</p> required <code>str</code> <p>The name of the column in the dataframe containing the edited text.</p> required <code>str | list[str]</code> <p>One or more column names acting as unique identifiers for each entry. If multiple entries are found with the same <code>entry_ids</code>, they are all treated as edits of the same text.</p> required <code>list[str]</code> <p>A list of columns containing additional information for each entry.</p> <code>[]</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict[str, Any]</code> <p>description. Defaults to {}.</p> <code>{}</code> <code>bool</code> <p>Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of insertions and deletions in the original/edited texts, respectively. If false, those are merged to the next token to the right. Default: True.</p> <code>True</code> <code>str</code> <p>The label for substitutions. Default: \"S\".</p> <code>'S'</code> <code>str</code> <p>The label for insertions. Default: \"I\".</p> <code>'I'</code> <code>str</code> <p>The label for deletions. Default: \"D\".</p> <code>'D'</code> <code>str</code> <p>The token to use for gaps. Default: \"\u2581\".</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>EditedDataset</code> <p>An <code>EditedDataset</code> initialized from the set of texts and edits.</p> Source code in <code>labl/data/edited_dataset.py</code> <pre><code>@classmethod\ndef from_edits_dataframe(\n    cls,\n    df,\n    text_column: str,\n    edit_column: str,\n    entry_ids: str | list[str],\n    infos_columns: list[str] = [],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; \"EditedDataset\":\n    \"\"\"Create an `EditedDataset` from a `pandas.DataFrame` with edits.\n\n    Every row in the DataFrame is an entry identified univocally by `entry_ids`. The `text_column` contains the\n    original text, and the `edit_column` contains the edits. If multiple columns with the same `entry_ids` are\n    present, they are all treated as edits of the same text.\n\n    Args:\n        df (pandas.DataFrame): The DataFrame containing the text and edits.\n        text_column (str): The name of the column in the dataframe containing the original text.\n        edit_column (str): The name of the column in the dataframe containing the edited text.\n        entry_ids (str | list[str]): One or more column names acting as unique identifiers for each entry. If\n            multiple entries are found with the same `entry_ids`, they are all treated as edits of the same text.\n        infos_columns (list[str]): A list of columns containing additional information for each entry.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None, optional): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict[str, Any], optional): _description_. Defaults to {}.\n        with_gaps (bool): Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of\n            insertions and deletions in the original/edited texts, respectively. If false, those are merged to the\n            next token to the right. Default: True.\n        sub_label (str): The label for substitutions. Default: \"S\".\n        ins_label (str): The label for insertions. Default: \"I\".\n        del_label (str): The label for deletions. Default: \"D\".\n        gap_token (str): The token to use for gaps. Default: \"\u2581\".\n\n    Returns:\n        An `EditedDataset` initialized from the set of texts and edits.\n    \"\"\"\n    if not is_pandas_available():\n        raise ImportError(\"Pandas is not installed. Please install pandas to use this function.\")\n    import pandas as pd\n\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    df = cast(pd.DataFrame, df)\n    if isinstance(entry_ids, str):\n        entry_ids = [entry_ids]\n    grouped_dfs = df.groupby(entry_ids).size().reset_index()\n    all_texts = []\n    all_edits = []\n    all_infos = []\n    for _, entry_row in tqdm(\n        grouped_dfs.iterrows(), desc=\"Extracting texts and edits\", total=len(grouped_dfs), unit=\"entries\"\n    ):\n        curr_vals = [entry_row[col] for col in entry_ids]\n        edit_rows = df[(df[entry_ids] == curr_vals).all(axis=1)]\n        text = edit_rows[text_column].tolist()[0]\n        edits = edit_rows[edit_column].tolist()\n        all_texts.append(text)\n        all_edits.append(edits)\n        infos = []\n        for _, edit_row in edit_rows.iterrows():\n            infos.append({col: edit_row[col] for col in infos_columns})\n        all_infos.append(infos)\n    return EditedDataset.from_edits(\n        all_texts,\n        all_edits,\n        all_infos,\n        tokenizer=tokenizer,\n        with_gaps=with_gaps,\n        sub_label=sub_label,\n        ins_label=ins_label,\n        del_label=del_label,\n        gap_token=gap_token,\n    )\n</code></pre>"},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(df)","title":"<code>df</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(text_column)","title":"<code>text_column</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(edit_column)","title":"<code>edit_column</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(entry_ids)","title":"<code>entry_ids</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(infos_columns)","title":"<code>infos_columns</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(with_gaps)","title":"<code>with_gaps</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(sub_label)","title":"<code>sub_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(ins_label)","title":"<code>ins_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(del_label)","title":"<code>del_label</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.from_edits_dataframe(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/data/dataset/#labl.data.edited_dataset.EditedDataset.merge_gap_annotations","title":"merge_gap_annotations","text":"<pre><code>merge_gap_annotations(\n    merge_fn: Callable[[Sequence[LabelType]], LabelType]\n    | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None\n</code></pre> <p>Merge gap annotations in the tokens of <code>orig</code> and <code>edit</code>.</p> <p>This method is equivalent to calling <code>EditedEntry.from_edits</code> with <code>with_gaps=False</code>. Gap annotations are merged to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last gap is kept to account for insertions at the end of the text.</p> <p>E.g. <code>GAP Hello GAP World GAP ! GAP</code> becomes <code>Hello World ! GAP</code>.      <code>I     S   I               I</code> <code>IS     I     I</code></p> Source code in <code>labl/data/edited_dataset.py</code> <pre><code>def merge_gap_annotations(\n    self,\n    merge_fn: Callable[[Sequence[LabelType]], LabelType] | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None:\n    \"\"\"Merge gap annotations in the tokens of `orig` and `edit`.\n\n    This method is equivalent to calling `EditedEntry.from_edits` with `with_gaps=False`. Gap annotations are merged\n    to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last\n    gap is kept to account for insertions at the end of the text.\n\n    E.g. `GAP Hello GAP World GAP ! GAP` becomes `Hello World ! GAP`.\n         `  I     S   I               I`         `   IS     I     I`\n    \"\"\"\n    for entry in self:\n        cast(EditedEntry | MultiEditEntry, entry).merge_gap_annotations(\n            merge_fn=merge_fn, keep_final_gap=keep_final_gap\n        )\n</code></pre>"},{"location":"api/data/entry/","title":"Entry","text":""},{"location":"api/data/entry/#labl.data.base_entry.BaseLabeledEntry","title":"labl.data.base_entry.BaseLabeledEntry","text":"<p>               Bases: <code>LabeledInterface</code>, <code>ABC</code></p> <p>Base class for all data entries. This class handles the creation of public getters, disallowing setting and providing a private constructor key to prevent direct instantiation.</p>"},{"location":"api/data/entry/#labl.data.base_entry.BaseLabeledEntry.relabel","title":"relabel","text":"<pre><code>relabel(\n    relabel_fn: Callable[[LabelType], LabelType]\n    | None = None,\n    relabel_map: dict[str | int, LabelType] | None = None,\n) -&gt; None\n</code></pre> <p>Relabels the entry in-place using a custom relabeling function or a mapping.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[[str | int | float | None], str | int | float | None]</code> <p>A function that will be applied to each label in the entry. The function should take a single argument (the label) and return the new label. The function should return the label without any processing if the label should be preserved.</p> <code>None</code> <code>dict[str | int, str | int | float | None]</code> <p>A dictionary that maps old labels to new labels. The keys are the old labels and the values are the new labels. This can be used instead of the relabel_fn to relabel the entry if labels are discrete.</p> <code>None</code> Source code in <code>labl/data/base_entry.py</code> <pre><code>def relabel(\n    self,\n    relabel_fn: Callable[[LabelType], LabelType] | None = None,\n    relabel_map: dict[str | int, LabelType] | None = None,\n) -&gt; None:\n    \"\"\"Relabels the entry in-place using a custom relabeling function or a mapping.\n\n    Args:\n        relabel_fn (Callable[[str | int | float | None], str | int | float | None]):\n            A function that will be applied to each label in the entry.\n            The function should take a single argument (the label) and return the new label.\n            The function should return the label without any processing if the label should be preserved.\n        relabel_map (dict[str | int, str | int | float | None]):\n            A dictionary that maps old labels to new labels. The keys are the old labels and the values are the\n            new labels. This can be used instead of the relabel_fn to relabel the entry if labels are discrete.\n    \"\"\"\n    if relabel_fn is None:\n        if relabel_map is None:\n            raise ValueError(\"Either relabel_fn or relabel_map must be provided.\")\n        relabel_fn = lambda x: x if x is None or isinstance(x, float) else relabel_map.get(x, x)\n    self._relabel_attributes(relabel_fn)\n    self._label_types = self._get_label_types()\n</code></pre>"},{"location":"api/data/entry/#labl.data.base_entry.BaseLabeledEntry.relabel(relabel_fn)","title":"<code>relabel_fn</code>","text":""},{"location":"api/data/entry/#labl.data.base_entry.BaseLabeledEntry.relabel(relabel_map)","title":"<code>relabel_map</code>","text":""},{"location":"api/data/entry/#labl.data.base_entry.BaseLabeledEntry.get_agreement","title":"get_agreement","text":"<pre><code>get_agreement(\n    other: EntryType,\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput\n</code></pre> <p>Compute the inter-annotator agreement for the token labels of two entries using Krippendorff's alpha.</p> Source code in <code>labl/data/base_entry.py</code> <pre><code>def get_agreement(\n    self: EntryType,\n    other: EntryType,\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput:\n    \"\"\"Compute the inter-annotator agreement for the token labels of two entries using\n    [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha).\n    \"\"\"\n    self._validate_single_label_type()\n    other._validate_single_label_type()\n    if self.label_types[0] != other.label_types[0]:\n        raise RuntimeError(\n            f\"Label type does not match: {self.label_types[0]} vs {other.label_types[0]}.\\n\"\n            \"Transform the annotations using `.relabel` to ensure a single type is present.\"\n        )\n    labels_array = self._get_labels_array([self, other], dtype=self.label_types[0])\n    return get_labels_agreement(\n        label_type=self.label_types[0],\n        labels_array=labels_array,\n        level_of_measurement=level_of_measurement,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry","title":"labl.data.labeled_entry.LabeledEntry","text":"<pre><code>LabeledEntry(\n    text: str,\n    spans: SpanList,\n    tagged: str,\n    tokens: list[str],\n    tokens_labels: Sequence[LabelType],\n    tokens_offsets: list[OffsetType],\n    info: InfoDictType = {},\n    constructor_key: object | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseLabeledEntry</code></p> <p>Class for a text entry with a set of granular annotations over some of its parts.</p> <p>The class provides a centralized object to easily switch between different annotation formats:</p> <ul> <li>The original text without labels.</li> <li>A tagged version with ... labels.</li> <li>A list of spans corresponding to tagged substrings, with start/end indices.</li> <li>A tokenized version of the text with labels associated to each token.</li> </ul> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The original text.</p> <code>spans</code> <code>list[Span]</code> <p>A list of lists of <code>Span</code> items containing the start/end indices of the span, the contained text and a label associated to it.</p> <code>tagged</code> <code>str</code> <p>A tagged versions of <code>text</code> containing tags like <code>&lt;tag&gt;...&lt;/tag&gt;</code> to mark labels from <code>spans</code> or <code>tokens</code>.</p> <code>tokens</code> <code>list[str]</code> <p>A list of strings representing the tokenized version of <code>text</code>.</p> <code>tokens_labels</code> <code>list[str | int | float | None]</code> <p>A list of the same length as <code>tokens</code> containing labels associated with every token. Labels can be strings, numbers or <code>None</code>.</p> <code>labeled_tokens</code> <code>list[list[LabeledToken]]</code> <p>A list of <code>LabeledToken</code> objects joining <code>tokens</code> and <code>tokens_labels</code>.</p> <code>tokens_offsets</code> <code>list[tuple[int, int] | None]</code> <p>Offsets for each token in <code>tokens</code>. The i-th element corresponds to the i-th token in <code>tokens</code>. The offsets are tuples of the form <code>(start, end)</code> corresponding to start and end positions of the token in <code>text</code>. If the token does not exist in <code>text</code>, the offset is <code>None</code>.</p> <code>label_types</code> <code>list[type]</code> <p>A list of the types of labels for the entry.</p> <code>info</code> <code>dict[str, str | int | float | bool]</code> <p>A dictionary containing additional information about the entry.</p> <p>A <code>LabeledEntry</code> can be initialized from:</p> <ul> <li> <p>A <code>tagged</code> text, e.g. <code>Hello &lt;error&gt;world&lt;/error&gt;!</code>, using <code>LabeledEntry.from_tagged(tagged=...)</code>.</p> </li> <li> <p>A <code>text</code> and a list of labeled <code>spans</code>, e.g. <code>Hello world!</code> and <code>[{'start': 0, 'end': 5, 'label': 'error'}]</code>,     using <code>LabeledEntry.from_spans(text=..., spans=...)</code>.</p> </li> <li> <p>A list of <code>labeled_tokens</code> with string/numeric labels, e.g. <code>[('Hel', 0.5), ('lo', 0.7), ('world', 1),     ('!', 0)]</code>, or two separate lists of <code>tokens</code> and <code>labels</code> using <code>LabeledEntry.from_tokens(labeled_tokens=...)</code>     or <code>LabeledEntry.from_tokens(tokens=..., labels=)</code>.</p> </li> </ul> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>def __init__(\n    self,\n    text: str,\n    spans: SpanList,\n    tagged: str,\n    tokens: list[str],\n    tokens_labels: Sequence[LabelType],\n    tokens_offsets: list[OffsetType],\n    info: InfoDictType = {},\n    constructor_key: object | None = None,\n):\n    \"\"\"Private constructor for `LabeledEntry`.\n\n    A `LabeledEntry` can be initialized from:\n\n    * A `tagged` text, e.g. `Hello &lt;error&gt;world&lt;/error&gt;!`, using `LabeledEntry.from_tagged(tagged=...)`.\n\n    * A `text` and a list of labeled `spans`, e.g. `Hello world!` and `[{'start': 0, 'end': 5, 'label': 'error'}]`,\n        using `LabeledEntry.from_spans(text=..., spans=...)`.\n\n    * A list of `labeled_tokens` with string/numeric labels, e.g. `[('Hel', 0.5), ('lo', 0.7), ('world', 1),\n        ('!', 0)]`, or two separate lists of `tokens` and `labels` using `LabeledEntry.from_tokens(labeled_tokens=...)`\n        or `LabeledEntry.from_tokens(tokens=..., labels=)`.\n    \"\"\"\n    if constructor_key != self.__constructor_key:\n        raise RuntimeError(\n            dedent(\"\"\"\\\n            The default constructor for `LabeledEntry` is private. A `LabeledEntry` can be initialized from:\n\n            * A `tagged` text, e.g. `Hello &lt;error&gt;world&lt;/error&gt;!`, using `LabeledEntry.from_tagged(tagged=...)`.\n\n            * A `text` and a list of labeled `spans`, e.g. `Hello world!` and `[{'start': 0, 'end': 5, 'label': 'error'}]`,\n                using `LabeledEntry.from_spans(text=..., spans=...)`.\n\n            * A list of `labeled_tokens` with string/numeric labels, e.g. `[('Hel', 0.5), ('lo', 0.7), ('world', 1),\n                ('!', 0)]`, or two separate lists of `tokens` and `labels` using `LabeledEntry.from_tokens(labeled_tokens=...)`\n                or `LabeledEntry.from_tokens(tokens=..., labels=)`.\n            \"\"\")\n        )\n    self._text = text\n    self._spans = spans\n    self._tagged = tagged\n    self._tokens = tokens\n    self._tokens_labels = tokens_labels\n    self._tokens_offsets = tokens_offsets\n    self._info = info\n    self._label_types = self._get_label_types()\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.text","title":"text  <code>property</code> <code>writable</code>","text":"<pre><code>text: str\n</code></pre> <p>The input text. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.spans","title":"spans  <code>property</code> <code>writable</code>","text":"<pre><code>spans: SpanList\n</code></pre> <p>Labeled spans of the text. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.tagged","title":"tagged  <code>property</code> <code>writable</code>","text":"<pre><code>tagged: str\n</code></pre> <p>The tagged version of the text. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.tokens","title":"tokens  <code>property</code> <code>writable</code>","text":"<pre><code>tokens: list[str]\n</code></pre> <p>The tokenized version of the text. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.tokens_labels","title":"tokens_labels  <code>property</code> <code>writable</code>","text":"<pre><code>tokens_labels: Sequence[LabelType]\n</code></pre> <p>The labels associated with the tokens. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.tokens_offsets","title":"tokens_offsets  <code>property</code> <code>writable</code>","text":"<pre><code>tokens_offsets: list[OffsetType]\n</code></pre> <p>The offsets for each token in the text. This is a read-only property.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.labeled_tokens","title":"labeled_tokens  <code>property</code> <code>writable</code>","text":"<pre><code>labeled_tokens: LabeledTokenList\n</code></pre> <p>Returns a list of <code>LabeledToken</code> objects joining <code>tokens</code> and <code>tokens_labels</code> with custom visualization.</p>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans","title":"from_spans  <code>classmethod</code>","text":"<pre><code>from_spans(\n    text: str,\n    spans: list[Span] | list[SpanType],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; LabeledEntry\n</code></pre> <p>Create a <code>LabeledEntry</code> from a text and a list of spans.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The original text.</p> required <code>list[Span] | list[dict[str, str | int | float | None]]</code> <p>A list or a list of lists of <code>Span</code> items or equivalent dicts containing information about specific     spans in <code>text</code>.</p> required <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>dict[str, str | int | float | bool]</code> <p>A dictionary containing additional information about the entry.</p> <code>{}</code> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@classmethod\ndef from_spans(\n    cls,\n    text: str,\n    spans: list[Span] | list[SpanType],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; \"LabeledEntry\":\n    \"\"\"Create a `LabeledEntry` from a text and a list of spans.\n\n    Args:\n        text (str):\n            The original text.\n        spans (list[Span] | list[dict[str, str | int | float | None]]):\n            A list or a list of lists of `Span` items or equivalent dicts containing information about specific\n                spans in `text`.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n        info (dict[str, str | int | float | bool]):\n            A dictionary containing additional information about the entry.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    spans = Span.from_list(spans)\n    tokens, tokens_labels, tokens_offsets = cls.get_tokens_from_spans(text=text, spans=spans, tokenizer=tokenizer)\n    tagged = cls.get_tagged_from_spans(text=text, spans=spans)\n    return cls(\n        text=text,\n        spans=spans,\n        tagged=tagged,\n        tokens=tokens,\n        tokens_labels=tokens_labels,\n        tokens_offsets=tokens_offsets,\n        info=info,\n        constructor_key=cls.__constructor_key,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans(spans)","title":"<code>spans</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_spans(info)","title":"<code>info</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged","title":"from_tagged  <code>classmethod</code>","text":"<pre><code>from_tagged(\n    tagged: str,\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; LabeledEntry\n</code></pre> <p>Create a <code>LabeledEntry</code> from a tagged text.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Tagged version of <code>text</code>.</p> required <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>list[str]</code> <p>Tag(s) used to mark selected spans, e.g. <code>h</code> for tags like <code>&lt;h&gt;...&lt;/h&gt;</code>. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <code>list[str]</code> <p>Tag(s) that are present in the text but should be ignored while parsing. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>dict[str, str | int | float | bool]</code> <p>A dictionary containing additional information about the entry.</p> <code>{}</code> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@classmethod\ndef from_tagged(\n    cls,\n    tagged: str,\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; \"LabeledEntry\":\n    \"\"\"Create a `LabeledEntry` from a tagged text.\n\n    Args:\n        tagged (str): Tagged version of `text`.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        keep_tags (list[str]):\n            Tag(s) used to mark selected spans, e.g. `h` for tags like `&lt;h&gt;...&lt;/h&gt;`. If not provided, all\n            tags are kept (Default: []).\n        ignore_tags (list[str]):\n            Tag(s) that are present in the text but should be ignored while parsing. If not provided, all tags\n            are kept (Default: []).\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n        info (dict[str, str | int | float | bool]):\n            A dictionary containing additional information about the entry.\n    \"\"\"\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    text, spans = cls.get_text_and_spans_from_tagged(tagged=tagged, keep_tags=keep_tags, ignore_tags=ignore_tags)\n    tokens, tokens_labels, tokens_offsets = cls.get_tokens_from_spans(text=text, spans=spans, tokenizer=tokenizer)\n    return cls(\n        text=text,\n        spans=spans,\n        tagged=tagged,\n        tokens=tokens,\n        tokens_labels=tokens_labels,\n        tokens_offsets=tokens_offsets,\n        info=info,\n        constructor_key=cls.__constructor_key,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(tagged)","title":"<code>tagged</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(keep_tags)","title":"<code>keep_tags</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(ignore_tags)","title":"<code>ignore_tags</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tagged(info)","title":"<code>info</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens","title":"from_tokens  <code>classmethod</code>","text":"<pre><code>from_tokens(\n    tokens: list[str],\n    labels: Sequence[LabelType],\n    text: str | None = None,\n    offsets: list[OffsetType] | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; LabeledEntry\n</code></pre> <p>Create a <code>LabeledEntry</code> from a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>list[str] | None</code> <p>A list of tokens. Can be provided together with <code>labels</code> as an alternative to <code>labeled_tokens</code>.</p> required <code>list[str | int | float | None] | None</code> <p>A list of labels for the tokens. Can be provided together with <code>tokens</code> as an alternative to <code>labeled_tokens</code>.</p> required <code>str | None</code> <p>The original text. If not provided, it is detokenized from <code>tokens</code> using the tokenizer.</p> <code>None</code> <code>list[tuple[int, int] | None] | None</code> <p>The offsets for each token in <code>tokens</code>. The i-th element corresponds to the i-th token in <code>tokens</code>. The offsets are tuples of the form <code>(start, end)</code> corresponding to start and end positions of the token in <code>text</code>. If the token does not exist in <code>text</code>, the offset is <code>None</code>. If not provided, it is computed using the tokenizer.</p> <code>None</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>list[str]</code> <p>Label(s) used to mark selected tokens. If not provided, all labels are kept (Default: []).</p> <code>[]</code> <code>list[str]</code> <p>Label(s) that are present on tokens but should be ignored while parsing. If not provided, all labels are kept (Default: []).</p> <code>[]</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>dict[str, str | int | float | bool]</code> <p>A dictionary containing additional information about the entry.</p> <code>{}</code> Example <pre><code>from labl.data.labeled_entry import LabeledEntry\n\nentry = LabeledEntry.from_tokens(\n    labeled_tokens=[\n        (\"Apple\", \"ORG\"), (\"Inc.\", \"ORG\"), (\"is\", \"O\"), (\"looking\", \"O\"),\n        (\"at\", \"O\"), (\"buying\", \"O\"), (\"U.K.\", \"LOC\"), (\"startup\", \"O\"),\n        (\"for\", \"O\"), (\"$1\", \"MONEY\"), (\"billion\", \"MONEY\")\n    ],\n    ignore_labels=[\"O\"],\n)\nprint(entry.tokens)\n&gt;&gt;&gt; Apple Inc. is looking at buying U.K. startup for    $1 billion\n      ORG  ORG                       LOC             MONEY   MONEY\n</code></pre> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@classmethod\ndef from_tokens(\n    cls,\n    tokens: list[str],\n    labels: Sequence[LabelType],\n    text: str | None = None,\n    offsets: list[OffsetType] | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n    info: InfoDictType = {},\n) -&gt; \"LabeledEntry\":\n    \"\"\"Create a `LabeledEntry` from a list of tokens.\n\n    Args:\n        tokens (list[str] | None):\n            A list of tokens. Can be provided together with `labels` as an alternative to `labeled_tokens`.\n        labels (list[str | int | float | None] | None):\n            A list of labels for the tokens. Can be provided together with `tokens` as an alternative to\n            `labeled_tokens`.\n        text (str | None):\n            The original text. If not provided, it is detokenized from `tokens` using the tokenizer.\n        offsets (list[tuple[int, int] | None] | None):\n            The offsets for each token in `tokens`. The i-th element corresponds to the i-th token in `tokens`.\n            The offsets are tuples of the form `(start, end)` corresponding to start and end positions of the\n            token in `text`. If the token does not exist in `text`, the offset is `None`. If not provided, it is\n            computed using the tokenizer.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        keep_labels (list[str]):\n            Label(s) used to mark selected tokens. If not provided, all labels are kept (Default: []).\n        ignore_labels (list[str]):\n            Label(s) that are present on tokens but should be ignored while parsing. If not provided, all labels\n            are kept (Default: []).\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n        info (dict[str, str | int | float | bool]):\n            A dictionary containing additional information about the entry.\n\n    Example:\n        ```python\n        from labl.data.labeled_entry import LabeledEntry\n\n        entry = LabeledEntry.from_tokens(\n            labeled_tokens=[\n                (\"Apple\", \"ORG\"), (\"Inc.\", \"ORG\"), (\"is\", \"O\"), (\"looking\", \"O\"),\n                (\"at\", \"O\"), (\"buying\", \"O\"), (\"U.K.\", \"LOC\"), (\"startup\", \"O\"),\n                (\"for\", \"O\"), (\"$1\", \"MONEY\"), (\"billion\", \"MONEY\")\n            ],\n            ignore_labels=[\"O\"],\n        )\n        print(entry.tokens)\n        &gt;&gt;&gt; Apple Inc. is looking at buying U.K. startup for    $1 billion\n              ORG  ORG                       LOC             MONEY   MONEY\n        ```\n    \"\"\"\n    if len(tokens) != len(labels):\n        raise RuntimeError(\"The length of `tokens` and `labels` must be the same. \")\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    if text is None:\n        text = tokenizer.detokenize(tokens)[0]\n    if offsets is None:\n        _, all_offsets = tokenizer.tokenize_with_offsets(text)\n        offsets = all_offsets[0]\n    spans = cls.get_spans_from_tokens(text, labels, offsets, tokenizer, keep_labels, ignore_labels)\n    tagged = cls.get_tagged_from_spans(text, spans=spans)\n    return cls(\n        text=text,\n        spans=spans,\n        tagged=tagged,\n        tokens=tokens,\n        tokens_labels=labels,\n        tokens_offsets=offsets,\n        info=info,\n        constructor_key=cls.__constructor_key,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(labels)","title":"<code>labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(offsets)","title":"<code>offsets</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(keep_labels)","title":"<code>keep_labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(ignore_labels)","title":"<code>ignore_labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_tokens(info)","title":"<code>info</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tagged_from_spans","title":"get_tagged_from_spans  <code>staticmethod</code>","text":"<pre><code>get_tagged_from_spans(text: str, spans: list[Span]) -&gt; str\n</code></pre> <p>Tags one or more texts using lists of spans.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to which tags should be added.</p> required <code>list[Span]</code> <p>The spans to convert to tags.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The tagged texts.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@staticmethod\ndef get_tagged_from_spans(\n    text: str,\n    spans: list[Span],\n) -&gt; str:\n    \"\"\"Tags one or more texts using lists of spans.\n\n    Args:\n        text (str): The text to which tags should be added.\n        spans (list[Span]): The spans to convert to tags.\n\n    Returns:\n        The tagged texts.\n    \"\"\"\n    if not spans:\n        return text\n    tagged = text\n    sorted_spans = sorted(spans, key=lambda s: s.start)\n    offset = 0\n    for s in sorted_spans:\n        if s.label:\n            start = s.start + offset\n            end = s.end + offset\n            label = s.label\n            tagged = f\"{tagged[:start]}&lt;{label}&gt;{tagged[start:end]}&lt;/{label}&gt;{tagged[end:]}\"\n\n            # Update the offset for the next span\n            offset += len(str(label)) * 2 + 5  # &lt;{label}&gt;...&lt;/{label}&gt;\n    return tagged\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tagged_from_spans(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tagged_from_spans(spans)","title":"<code>spans</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tokens_from_spans","title":"get_tokens_from_spans  <code>staticmethod</code>","text":"<pre><code>get_tokens_from_spans(\n    text: str,\n    spans: list[Span],\n    tokenizer: Tokenizer | None = None,\n) -&gt; tuple[\n    list[str], Sequence[LabelType], list[OffsetType]\n]\n</code></pre> <p>Extracts tokens, labels and offsets from a text and a set of labeled spans.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The text to which tags should be added.</p> required <code>list[Span]</code> <p>The spans to convert to tokens.</p> required <code>Tokenizer | None</code> <p>A <code>Tokenizer</code> used for text splitting. If not provided, whitespace tokenization is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[str], Sequence[LabelType], list[OffsetType]]</code> <p>A tuple <code>(tokens, tokens_labels, tokens_offsets)</code>, which are three lists of the same length containing respectively the tokens, their labels and their (start, end) offsets.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@staticmethod\ndef get_tokens_from_spans(\n    text: str,\n    spans: list[Span],\n    tokenizer: Tokenizer | None = None,\n) -&gt; tuple[list[str], Sequence[LabelType], list[OffsetType]]:\n    \"\"\"Extracts tokens, labels and offsets from a text and a set of labeled spans.\n\n    Args:\n        text (str): The text to which tags should be added.\n        spans (list[Span]): The spans to convert to tokens.\n        tokenizer (Tokenizer | None): A `Tokenizer` used for text splitting. If not provided, whitespace\n            tokenization is used.\n\n    Returns:\n        A tuple `(tokens, tokens_labels, tokens_offsets)`, which are three lists of the same length containing\n            respectively the tokens, their labels and their (start, end) offsets.\n    \"\"\"\n    if tokenizer is None:\n        logger.info(\"Tokenizer was not provided. Defaulting to whitespace tokenization.\")\n        tokenizer = WhitespaceTokenizer()\n    all_tokens, all_tokens_offsets = tokenizer.tokenize_with_offsets(text)\n    tokens, tokens_offsets = all_tokens[0], all_tokens_offsets[0]\n    if not spans:\n        return tokens, [None] * len(tokens), tokens_offsets\n    sorted_spans = sorted(spans, key=lambda s: s.start)\n\n    # Pointer for the current position in sorted_spans\n    span_idx = 0\n    tokens_labels = []\n\n    for offset in tokens_offsets:\n        if offset is None:\n            tokens_labels.append(None)\n            continue\n        token_start, token_end = offset\n        label = None\n\n        # Skip spans that end before the token starts\n        while span_idx &lt; len(sorted_spans) and sorted_spans[span_idx].end &lt;= token_start:\n            span_idx += 1\n\n        # Iterate through spans starting from the current span_idx, as long as\n        # the span starts before the current token ends. If a span starts\n        # at or after the token ends, it (and all subsequent spans) cannot overlap.\n        current_check_idx = span_idx\n        while current_check_idx &lt; len(sorted_spans) and sorted_spans[span_idx].start &lt; token_end:\n            span = sorted_spans[current_check_idx]\n\n            # Check for actual overlap using the standard condition:\n            # Does the interval [token_start, token_end) intersect with [span_start, span_end)?\n            # Overlap = max(start1, start2) &lt; min(end1, end2)\n            if max(token_start, span.start) &lt; min(token_end, span.end):\n                if label is None:\n                    label = span.label\n                else:\n                    label += span.label  # type: ignore\n            current_check_idx += 1  # Move to the next potentially overlapping span\n        tokens_labels.append(label)\n    return tokens, tokens_labels, tokens_offsets\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tokens_from_spans(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tokens_from_spans(spans)","title":"<code>spans</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tokens_from_spans(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_text_and_spans_from_tagged","title":"get_text_and_spans_from_tagged  <code>staticmethod</code>","text":"<pre><code>get_text_and_spans_from_tagged(\n    tagged: str,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n) -&gt; tuple[str, SpanList]\n</code></pre> <p>Extract spans and clean text from a tagged string.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The tagged string to extract spans from.</p> required <code>list[str]</code> <p>Tag(s) used to mark selected spans, e.g. <code>&lt;h&gt;...&lt;/h&gt;</code>, <code>&lt;error&gt;...&lt;/error&gt;</code>. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <code>list[str]</code> <p>Tag(s) that are present in the text but should be ignored while parsing. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <p>Returns:</p> Type Description <code>tuple[str, SpanList]</code> <p>Tuple containing the cleaned text and a list of <code>Span</code> objects.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@staticmethod\ndef get_text_and_spans_from_tagged(\n    tagged: str,\n    keep_tags: list[str] = [],\n    ignore_tags: list[str] = [],\n) -&gt; tuple[str, SpanList]:\n    \"\"\"Extract spans and clean text from a tagged string.\n\n    Args:\n        tagged (str): The tagged string to extract spans from.\n        keep_tags (list[str]):\n            Tag(s) used to mark selected spans, e.g. `&lt;h&gt;...&lt;/h&gt;`, `&lt;error&gt;...&lt;/error&gt;`. If not provided,\n            all tags are kept (Default: []).\n        ignore_tags (list[str]):\n            Tag(s) that are present in the text but should be ignored while parsing. If not provided,\n            all tags are kept (Default: []).\n\n    Returns:\n        Tuple containing the cleaned text and a list of `Span` objects.\n    \"\"\"\n    any_tag_regex = re.compile(r\"&lt;\\/?(?:\\w+)&gt;\")\n    if not keep_tags:\n        tag_regex = any_tag_regex\n    else:\n        tag_match_string = \"|\".join(list(set(keep_tags) | set(ignore_tags)))\n        tag_regex = re.compile(rf\"&lt;\\/?(?:{tag_match_string})&gt;\")\n\n    text_without_tags: str = \"\"\n    spans = SpanList()\n    current_pos = 0\n    open_tags = []\n    open_positions = []\n\n    for match in tag_regex.finditer(tagged):\n        match_text = match.group(0)\n        start, end = match.span()\n\n        # Add text before the tag\n        text_without_tags += tagged[current_pos:start]\n        current_pos = end\n\n        # Check if opening or closing tag\n        if match_text.startswith(\"&lt;/\"):\n            tag_name = match_text[2:-1]\n            if not open_tags or open_tags[-1] != tag_name:\n                raise RuntimeError(f\"Closing tag {match_text} without matching opening tag\")\n\n            # Create span for the highlighted text\n            open_pos = open_positions.pop()\n            open_tag = open_tags.pop()\n            if tag_name not in ignore_tags:\n                tagged_span = Span(\n                    start=open_pos,\n                    end=len(text_without_tags),\n                    label=open_tag,\n                )\n                spans.append(tagged_span)\n        else:\n            # Opening tag\n            tag_name = match_text[1:-1]\n            if keep_tags and (tag_name not in keep_tags and tag_name not in ignore_tags):\n                raise RuntimeError(\n                    f\"Unexpected tag type: {tag_name}. \"\n                    \"Specify tag types that should be preserved in the `keep_tags` argument, \"\n                    \"and those that should be ignored in the `ignore_tag_types` argument.\"\n                )\n            open_tags.append(tag_name)\n            open_positions.append(len(text_without_tags))\n\n    # Add remaining text\n    text_without_tags += tagged[current_pos:]\n    if open_tags:\n        raise RuntimeError(f\"Unclosed tags: {', '.join(open_tags)}\")\n\n    # If the text contains a tag that was neither kept nor ignored, raise a warning\n    unexpected_tags = any_tag_regex.search(text_without_tags)\n    if unexpected_tags:\n        warn(\n            \"The text contains tag types that were not specified in keep_tags or ignore_tags: \"\n            f\"{unexpected_tags.group(0)}. These tags are now preserved in the output. If these should ignored \"\n            \"instead, add them to the `ignore_tags` argument.\",\n            stacklevel=2,\n        )\n    for span in spans:\n        span.text = text_without_tags[span.start : span.end]\n    return text_without_tags, spans\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_text_and_spans_from_tagged(tagged)","title":"<code>tagged</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_text_and_spans_from_tagged(keep_tags)","title":"<code>keep_tags</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_text_and_spans_from_tagged(ignore_tags)","title":"<code>ignore_tags</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens","title":"get_spans_from_tokens  <code>staticmethod</code>","text":"<pre><code>get_spans_from_tokens(\n    text: str,\n    labels: Sequence[LabelType],\n    offsets: list[OffsetType] | None = None,\n    tokenizer: Tokenizer | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n) -&gt; SpanList\n</code></pre> <p>Extract spans and clean text from a list of labeled tokens.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The original text.</p> required <code>list[str | int | float | None]</code> <p>The labels associated with the tokens.</p> required <code>list[tuple[int, int] | None] | None</code> <p>The offsets for each token in <code>tokens</code>. The i-th element corresponds to the i-th token in <code>tokens</code>.</p> <code>None</code> <code>Tokenizer | None</code> <p>The tokenizer to use for tokenization. If not provided, whitespace tokenization is used.</p> <code>None</code> <code>list[str]</code> <p>Token labels that should be ported over to spans. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <code>list[str]</code> <p>Token labels that should be ignored while parsing. If not provided, all tags are kept (Default: []).</p> <code>[]</code> <p>Returns:</p> Type Description <code>SpanList</code> <p>A list of <code>Span</code> objects corresponding to the labeled tokens.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@staticmethod\ndef get_spans_from_tokens(\n    text: str,\n    labels: Sequence[LabelType],\n    offsets: list[OffsetType] | None = None,\n    tokenizer: Tokenizer | None = None,\n    keep_labels: list[str] = [],\n    ignore_labels: list[str] = [],\n) -&gt; SpanList:\n    \"\"\"Extract spans and clean text from a list of labeled tokens.\n\n    Args:\n        text (str):\n            The original text.\n        labels (list[str | int | float | None]):\n            The labels associated with the tokens.\n        offsets (list[tuple[int, int] | None] | None):\n            The offsets for each token in `tokens`. The i-th element corresponds to the i-th token in `tokens`.\n        tokenizer (Tokenizer | None): The tokenizer to use for\n            tokenization. If not provided, whitespace tokenization is used.\n        keep_labels (list[str]):\n            Token labels that should be ported over to spans. If not provided, all tags are kept (Default: []).\n        ignore_labels (list[str]):\n            Token labels that should be ignored while parsing. If not provided, all tags are kept (Default: []).\n\n    Returns:\n        A list of `Span` objects corresponding to the labeled tokens.\n    \"\"\"\n    if offsets is None:\n        if tokenizer is None:\n            logger.info(\"Tokenizer was not provided. Defaulting to whitespace tokenization.\")\n            tokenizer = WhitespaceTokenizer()\n        _, all_offsets = tokenizer.tokenize_with_offsets(text)\n        offsets = all_offsets[0]\n    curr_span_label: LabelType = None\n    curr_span_start: int | None = None\n    curr_span_end: int | None = None\n    spans = SpanList()\n\n    # To be considered for a span, a token must have a valid label (not ignored) and a valid character span\n    # (not a special token).\n    for label, offset in zip(labels, offsets, strict=True):\n        is_ignored = label in ignore_labels\n        is_kept = not keep_labels or label in keep_labels\n        has_valid_label = is_kept and not is_ignored\n        if has_valid_label and offset is not None:\n            t_start, t_end = offset\n            if label == curr_span_label:\n                curr_span_end = t_end\n            else:\n                curr_span_label = label\n                curr_span_start = t_start\n                curr_span_end = t_end\n        else:\n            curr_span_label = None\n            curr_span_start = None\n            curr_span_end = None\n    if curr_span_label is not None and curr_span_start is not None and curr_span_end is not None:\n        spans.append(Span(start=curr_span_start, end=curr_span_end, label=curr_span_label))\n    for span in spans:\n        span.text = text[span.start : span.end]\n    return spans\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(labels)","title":"<code>labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(offsets)","title":"<code>offsets</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(keep_labels)","title":"<code>keep_labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_spans_from_tokens(ignore_labels)","title":"<code>ignore_labels</code>","text":""},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_tokens","title":"get_tokens","text":"<pre><code>get_tokens() -&gt; list[str]\n</code></pre> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>def get_tokens(self) -&gt; list[str]:\n    return self.tokens\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.get_labels","title":"get_labels","text":"<pre><code>get_labels() -&gt; Sequence[LabelType]\n</code></pre> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>def get_labels(self) -&gt; Sequence[LabelType]:\n    return self.tokens_labels\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; LabeledEntryDictType\n</code></pre> <p>Convert the <code>LabeledEntry</code> to a dictionary representation.</p> <p>Returns:</p> Type Description <code>LabeledEntryDictType</code> <p>A dictionary representation of the <code>LabeledEntry</code>.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>def to_dict(self) -&gt; LabeledEntryDictType:\n    \"\"\"Convert the `LabeledEntry` to a dictionary representation.\n\n    Returns:\n        A dictionary representation of the `LabeledEntry`.\n    \"\"\"\n    return LabeledEntryDictType(\n        {\n            \"_class\": self.__class__.__name__,\n            \"info\": self.info,\n            \"text\": self.text,\n            \"tagged\": self.tagged,\n            \"tokens\": self.tokens,\n            \"tokens_labels\": self.tokens_labels,\n            \"tokens_offsets\": self.tokens_offsets,\n            \"spans\": self.spans.to_dict(),\n        }\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: LabeledEntryDictType) -&gt; LabeledEntry\n</code></pre> <p>Create a <code>LabeledEntry</code> from a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary representation of the <code>LabeledEntry</code> obtained with <code>to_dict()</code>.</p> required <p>Returns:</p> Type Description <code>LabeledEntry</code> <p>A <code>LabeledEntry</code> object.</p> Source code in <code>labl/data/labeled_entry.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: LabeledEntryDictType) -&gt; \"LabeledEntry\":\n    \"\"\"Create a `LabeledEntry` from a dictionary representation.\n\n    Args:\n        data (dict): A dictionary representation of the `LabeledEntry` obtained with `to_dict()`.\n\n    Returns:\n        A `LabeledEntry` object.\n    \"\"\"\n    if \"_class\" not in data:\n        raise RuntimeError(\"The provided dictionary is missing the required _class attribute.\")\n    if data[\"_class\"] != cls.__name__:\n        raise RuntimeError(f\"Cannot load a {cls.__name__} object from {data['_class']}\")\n    return cls(\n        text=data[\"text\"],\n        spans=Span.from_list(data[\"spans\"]),\n        tagged=data[\"tagged\"],\n        tokens=data[\"tokens\"],\n        tokens_labels=data[\"tokens_labels\"],\n        tokens_offsets=data[\"tokens_offsets\"],\n        info=data[\"info\"],\n        constructor_key=cls.__constructor_key,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.labeled_entry.LabeledEntry.from_dict(data)","title":"<code>data</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry","title":"labl.data.edited_entry.EditedEntry","text":"<pre><code>EditedEntry(\n    orig: LabeledEntry,\n    edit: LabeledEntry,\n    has_gaps: bool,\n    has_bos_token: bool,\n    has_eos_token: bool,\n    aligned: WordOutput | None = None,\n    info: InfoDictType = {},\n    constructor_key: object | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseLabeledEntry</code></p> <p>Class for a pair of text entries (<code>orig</code> and <code>edit</code>) where word-level annotations are obtained from the aligned     tokens of the two entries.</p> <p>Attributes:</p> Name Type Description <code>orig</code> <code>LabeledEntry</code> <p>The original entry.</p> <code>edit</code> <code>LabeledEntry</code> <p>The edited entry.</p> <code>has_gaps</code> <code>bool</code> <p>Whether the token sequence has gaps. Gaps are used for text/edit pairs to mark the positions of insertions and deletions in the original/edited texts, respectively. If <code>False</code>, it means gap annotations were merged to the next token to the right.</p> <code>has_bos_token</code> <code>bool</code> <p>Whether the tokenizer has a beginning-of-sequence token.</p> <code>has_eos_token</code> <code>bool</code> <p>Whether the tokenizer has an end-of-sequence token.</p> <code>aligned</code> <code>WordOutput | None</code> <p>A <code>jiwer.WordOutput</code> with aligned tokens for <code>orig</code> and <code>edit</code>, using tokenized the provided tokenizer.</p> <code>info</code> <code>dict[str, str | int | float | bool]</code> <p>A dictionary containing additional information about the entry.</p> <p>One or more <code>EditedEntry</code> can be initialized from a  <code>text</code> and one or more <code>edits</code>, e.g. <code>Hello world!</code> and     <code>[\"Goodbye world!\", \"Hello planet!\"]</code>, using <code>EditedEntry.from_edits(text=..., edits=...)</code>.</p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def __init__(\n    self,\n    orig: LabeledEntry,\n    edit: LabeledEntry,\n    has_gaps: bool,\n    has_bos_token: bool,\n    has_eos_token: bool,\n    aligned: WordOutput | None = None,\n    info: InfoDictType = {},\n    constructor_key: object | None = None,\n):\n    \"\"\"Private constructor for `EditedEntry`.\n\n    One or more `EditedEntry` can be initialized from a  `text` and one or more `edits`, e.g. `Hello world!` and\n        `[\"Goodbye world!\", \"Hello planet!\"]`, using `EditedEntry.from_edits(text=..., edits=...)`.\n    \"\"\"\n    if constructor_key != self.__constructor_key:\n        raise RuntimeError(\n            dedent(\"\"\"\\\n            The default constructor for `EditedEntry` is private. One or more `EditedEntry` can be initialized from\n            a  `text` and one or more `edits`, e.g. `Hello world!` and `[\"Goodbye world!\", \"Hello planet!\"]`, using\n             `EditedEntry.from_edits(text=..., edits=...)`.\n            \"\"\")\n        )\n    self._orig = orig\n    self._edit = edit\n    self._aligned = aligned\n    self._has_gaps = has_gaps\n    self._has_bos_token = has_bos_token\n    self._has_eos_token = has_eos_token\n    self._label_types = list(set(self._orig._label_types) | set(self._edit._label_types))\n    self._info = info\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.orig","title":"orig  <code>property</code> <code>writable</code>","text":"<pre><code>orig: LabeledEntry\n</code></pre> <p>The <code>LabeledEntry</code> for the original text.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.edit","title":"edit  <code>property</code> <code>writable</code>","text":"<pre><code>edit: LabeledEntry\n</code></pre> <p>The <code>LabeledEntry</code> entry for the edited text.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.aligned","title":"aligned  <code>property</code> <code>writable</code>","text":"<pre><code>aligned: WordOutput | None\n</code></pre> <p>Aligned output using <code>jiwer</code> for <code>orig</code> and <code>edit</code>.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.has_gaps","title":"has_gaps  <code>property</code> <code>writable</code>","text":"<pre><code>has_gaps: bool\n</code></pre> <p>Boolean flag marking whether the token sequence has added gaps for insertion/deletion annotations.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.has_bos_token","title":"has_bos_token  <code>property</code> <code>writable</code>","text":"<pre><code>has_bos_token: bool\n</code></pre> <p>Boolean flag marking whether the tokenizer has a beginning-of-sequence token.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.has_eos_token","title":"has_eos_token  <code>property</code> <code>writable</code>","text":"<pre><code>has_eos_token: bool\n</code></pre> <p>Boolean flag marking whether the tokenizer has an end-of-sequence token.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.aligned_str","title":"aligned_str  <code>property</code> <code>writable</code>","text":"<pre><code>aligned_str: str\n</code></pre> <p>Aligned string at the token level with <code>jiwer.visualize_alignment</code>.</p>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits","title":"from_edits  <code>classmethod</code>","text":"<pre><code>from_edits(\n    text: str,\n    edits: str | list[str],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict = {},\n    with_gaps: bool = True,\n    keep_final_gap: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n    info: InfoDictType | list[InfoDictType] = {},\n) -&gt; EditedEntry | MultiEditEntry\n</code></pre> <p>Create a <code>EditedEntry</code> or an <code>MultiEditEntry</code> from a text and one or more edits.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The original text.</p> required <code>str | list[str] | None</code> <p>One or more edited version of the text.</p> required <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None</code> <p>A <code>Tokenizer</code> used for tokenization. Supports initialization from a <code>transformers.PreTrainedTokenizer</code>, and uses whitespace tokenization by default.</p> <code>None</code> <code>dict</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>bool</code> <p>Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of insertions and deletions in the original/edited texts, respectively. If false, those are merged to the next token to the right. Default: True.</p> <code>True</code> <code>bool</code> <p>Whether to keep the final gap token. Default: True.</p> <code>True</code> <code>str</code> <p>The label for substitutions. Default: \"S\".</p> <code>'S'</code> <code>str</code> <p>The label for insertions. Default: \"I\".</p> <code>'I'</code> <code>str</code> <p>The label for deletions. Default: \"D\".</p> <code>'D'</code> <code>str</code> <p>The token to use for gaps. Default: \"\u2581\".</p> <code>'\u2581'</code> <code>dict[str, str | int | float | bool] | list[dict[str, str | int | float | bool]]</code> <p>A dictionary containing additional information about the entry.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EditedEntry | MultiEditEntry</code> <p>A single <code>EditedEntry</code> if <code>edits</code> is a single string, otherwise an <code>MultiEditEntry</code> with one entry per edit.</p> Example <pre><code>from labl.data.edited_entry import EditedEntry\n\nentries = EditedEntry.from_edits(\n    text=\"a simple example\",\n    edits=[\"this is a simple enough test, you know?\", \"an example\"],\n    tokenizer=\"facebook/nllb-200-3.3B\",\n    tokenizer_kwargs={\n        \"tgt_lang\": \"ita_Latn\",\n        \"add_special_tokens\": True,\n    },\n)\nprint(entries[0].aligned_str)\n&gt;&gt;&gt; ORIG: ita_Latn ***** *** \u2581a \u2581simple ******* ***** * **** ***** \u2581example &lt;/s&gt;\n    EDIT: ita_Latn \u2581this \u2581is \u2581a \u2581simple \u2581enough \u2581test , \u2581you \u2581know        ? &lt;/s&gt;\n                       I   I                  I     I I    I     I        S\n</code></pre> Source code in <code>labl/data/edited_entry.py</code> <pre><code>@classmethod\ndef from_edits(\n    cls,\n    text: str,\n    edits: str | list[str],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict = {},\n    with_gaps: bool = True,\n    keep_final_gap: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n    info: InfoDictType | list[InfoDictType] = {},\n) -&gt; \"EditedEntry | MultiEditEntry\":\n    \"\"\"Create a `EditedEntry` or an `MultiEditEntry` from a text and one or more edits.\n\n    Args:\n        text (str): The original text.\n        edits (str | list[str] | None): One or more edited version of the text.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None): A `Tokenizer`\n            used for tokenization. Supports initialization from a `transformers.PreTrainedTokenizer`, and uses\n            whitespace tokenization by default.\n        tokenizer_kwargs (dict): Additional arguments for the tokenizer.\n        with_gaps (bool): Whether to add gaps to the tokens and offsets. Gaps are used to mark the positions of\n            insertions and deletions in the original/edited texts, respectively. If false, those are merged to the\n            next token to the right. Default: True.\n        keep_final_gap (bool): Whether to keep the final gap token. Default: True.\n        sub_label (str): The label for substitutions. Default: \"S\".\n        ins_label (str): The label for insertions. Default: \"I\".\n        del_label (str): The label for deletions. Default: \"D\".\n        gap_token (str): The token to use for gaps. Default: \"\u2581\".\n        info (dict[str, str | int | float | bool] | list[dict[str, str | int | float | bool]]):\n            A dictionary containing additional information about the entry.\n\n    Returns:\n        A single `EditedEntry` if `edits` is a single string, otherwise an `MultiEditEntry` with one entry per\n            edit.\n\n    Example:\n        ```python\n        from labl.data.edited_entry import EditedEntry\n\n        entries = EditedEntry.from_edits(\n            text=\"a simple example\",\n            edits=[\"this is a simple enough test, you know?\", \"an example\"],\n            tokenizer=\"facebook/nllb-200-3.3B\",\n            tokenizer_kwargs={\n                \"tgt_lang\": \"ita_Latn\",\n                \"add_special_tokens\": True,\n            },\n        )\n        print(entries[0].aligned_str)\n        &gt;&gt;&gt; ORIG: ita_Latn ***** *** \u2581a \u2581simple ******* ***** * **** ***** \u2581example &lt;/s&gt;\n            EDIT: ita_Latn \u2581this \u2581is \u2581a \u2581simple \u2581enough \u2581test , \u2581you \u2581know        ? &lt;/s&gt;\n                               I   I                  I     I I    I     I        S\n        ```\n    \"\"\"\n    edits = [edits] if isinstance(edits, str) else edits\n    if isinstance(info, list) and len(edits) != len(info):\n        raise RuntimeError(\n            f\"The number of edits ({len(edits)}) does not match the number of info dictionaries ({len(info)}).\"\n        )\n    tokenizer = get_tokenizer(tokenizer, tokenizer_kwargs)\n    tokens, offsets = tokenizer.tokenize_with_offsets(text)\n    tokens_with_gaps, offsets_with_gaps = tokenizer._add_gaps_to_tokens_and_offsets(tokens, offsets, gap_token)\n    tokens, offsets = tokens[0], offsets[0]\n    tokens_with_gaps, offsets_with_gaps = tokens_with_gaps[0], offsets_with_gaps[0]\n    all_edits_tokens, all_edits_offsets = tokenizer.tokenize_with_offsets(edits)\n    all_edits_tokens_with_gaps, all_edits_offsets_with_gaps = tokenizer._add_gaps_to_tokens_and_offsets(\n        all_edits_tokens, all_edits_offsets, gap_token=gap_token\n    )\n    entries = MultiEditEntry(info=info if isinstance(info, dict) else {})\n    all_info_dicts = info if isinstance(info, list) else [info] * len(edits)\n    for edit, e_tokens, e_offsets, e_tokens_with_gaps, e_offsets_with_gaps, e_info in zip(\n        edits,\n        all_edits_tokens,\n        all_edits_offsets,\n        all_edits_tokens_with_gaps,\n        all_edits_offsets_with_gaps,\n        all_info_dicts,\n        strict=True,\n    ):\n        aligned = process_words(\n            texts=[tokens], edits=[e_tokens], is_text_pre_transformed=True, is_edit_pre_transformed=True\n        )\n        tokens_labels, e_tokens_labels = cls.get_tokens_labels_from_edit(\n            text=text,\n            edit=edit,\n            tokens=tokens_with_gaps,\n            tokens_offsets=offsets_with_gaps,\n            edit_tokens=e_tokens_with_gaps,\n            edit_tokens_offsets=e_offsets_with_gaps,\n            aligned=aligned,\n            tokenizer=tokenizer,\n            sub_label=sub_label,\n            ins_label=ins_label,\n            del_label=del_label,\n            gap_token=gap_token,\n        )\n        if with_gaps:\n            out_tokens = tokens_with_gaps\n            out_offsets = offsets_with_gaps\n            out_edit_tokens = e_tokens_with_gaps\n            out_edit_offsets = e_offsets_with_gaps\n        else:\n            # If an ad-hoc EOS is added, it is always kept\n            if tokenizer.has_eos_token:\n                if not keep_final_gap:\n                    raise RuntimeError(\n                        \"The tokenizer has an EOS token, but `keep_final_gap` is set to False.\"\n                        \"The EOS token will be kept.\"\n                    )\n            tokens_labels = tokenizer._merge_gap_annotations(\n                [tokens_labels], has_bos_token=tokenizer.has_bos_token, keep_final_gap=keep_final_gap\n            )[0]\n            e_tokens_labels = tokenizer._merge_gap_annotations(\n                [e_tokens_labels], has_bos_token=tokenizer.has_bos_token, keep_final_gap=keep_final_gap\n            )[0]\n\n            # If gaps are merged, the last gap is kept regardless of it being a gap or not to mark end-insertions.\n            # If the tokenizer did not have an EOS token for that, the sequence will have an extra token and offsets\n            # will need to be adjusted.\n            if not keep_final_gap:\n                out_tokens, out_offsets = tokens, offsets\n                out_edit_tokens, out_edit_offsets = e_tokens, e_offsets\n            else:\n                out_tokens, out_offsets = tokens + [gap_token], offsets + [None]\n                out_edit_tokens, out_edit_offsets = e_tokens + [gap_token], e_offsets + [None]\n        entry = EditedEntry(\n            orig=LabeledEntry.from_tokens(\n                tokens=out_tokens,\n                labels=tokens_labels,\n                text=text,\n                offsets=out_offsets,\n                tokenizer=tokenizer,\n            ),\n            edit=LabeledEntry.from_tokens(\n                tokens=out_edit_tokens,\n                labels=e_tokens_labels,\n                text=edit,\n                offsets=out_edit_offsets,\n                tokenizer=tokenizer,\n            ),\n            aligned=aligned,\n            has_gaps=with_gaps,\n            has_bos_token=tokenizer.has_bos_token,\n            has_eos_token=tokenizer.has_eos_token,\n            info=e_info,\n            constructor_key=cls.__constructor_key,\n        )\n        entries.append(entry)\n    if len(entries) == 1:\n        return entries[0]\n    entries._label_types = entries._get_label_types()\n    return entries\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(edits)","title":"<code>edits</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(tokenizer_kwargs)","title":"<code>tokenizer_kwargs</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(with_gaps)","title":"<code>with_gaps</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(keep_final_gap)","title":"<code>keep_final_gap</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(sub_label)","title":"<code>sub_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(ins_label)","title":"<code>ins_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(del_label)","title":"<code>del_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_edits(info)","title":"<code>info</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit","title":"get_tokens_labels_from_edit  <code>classmethod</code>","text":"<pre><code>get_tokens_labels_from_edit(\n    text: str,\n    edit: str,\n    tokens: list[str] | None = None,\n    tokens_offsets: list[OffsetType] | None = None,\n    edit_tokens: list[str] | None = None,\n    edit_tokens_offsets: list[OffsetType] | None = None,\n    aligned: WordOutput | None = None,\n    tokenizer: Tokenizer | None = None,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[Sequence[str | None], Sequence[str | None]]\n</code></pre> <p>Convert text edits to token labels marking insertions, deletions and substitutions. The returned labels include gaps before/after each token, which can be merged to the right to match the original token sequence.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The original text.</p> required <code>str</code> <p>The edited text.</p> required <code>list[str] | None</code> <p>The tokenized version of <code>text</code>. If not provided, it will be computed using <code>tokenzier</code>. Default: <code>None</code>.</p> <code>None</code> <code>list[tuple[int, int] | None]</code> <p>The offsets of <code>tokens</code> in <code>text</code>. If not provided, it will be computed using <code>tokenzier</code>. Default: <code>None</code>.</p> <code>None</code> <code>list[str] | None</code> <p>The tokenized version of <code>edit</code>. If not provided, it will be computed using <code>tokenzier</code>. Default: <code>None</code>.</p> <code>None</code> <code>list[tuple[int, int] | None]</code> <p>The offsets of <code>edit_tokens</code> in <code>edit</code>. If not provided, it will be computed using <code>tokenzier</code>. Default: <code>None</code>.</p> <code>None</code> <code>WordOutput | None</code> <p>The aligned <code>WordOutput</code> between <code>text</code> and <code>edit</code>. If not provided, it will be obtained automatically using <code>tokenizer</code> for spltting. Default: <code>None</code>.</p> <code>None</code> <code>Tokenizer | None</code> <p>A <code>Tokenizer</code> used for text splitting. If not provided, whitespace tokenization is used by default. Default: <code>None</code>.</p> <code>None</code> <code>str</code> <p>The label for substitutions. Default: \"S\".</p> <code>'S'</code> <code>str</code> <p>The label for insertions. Default: \"I\".</p> <code>'I'</code> <code>str</code> <p>The label for deletions. Default: \"D\".</p> <code>'D'</code> <code>str</code> <p>The token to use for gaps. Default: \"\u2581\".</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>tuple[Sequence[str | None], Sequence[str | None]]</code> <p>A tuple containing two lists of labels (one for <code>text</code>, one for <code>edit</code>). Each label can be either None (if the token was not edited) or one of <code>sub_label</code>, <code>ins_label</code> or <code>del_label</code> depending on the type of operation associated with the token.</p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>@classmethod\ndef get_tokens_labels_from_edit(\n    cls,\n    text: str,\n    edit: str,\n    tokens: list[str] | None = None,\n    tokens_offsets: list[OffsetType] | None = None,\n    edit_tokens: list[str] | None = None,\n    edit_tokens_offsets: list[OffsetType] | None = None,\n    aligned: WordOutput | None = None,\n    tokenizer: Tokenizer | None = None,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[Sequence[str | None], Sequence[str | None]]:\n    \"\"\"Convert text edits to token labels marking insertions, deletions and substitutions. The returned labels\n    include gaps before/after each token, which can be merged to the right to match the original token sequence.\n\n    Args:\n        text (str): The original text.\n        edit (str): The edited text.\n        tokens (list[str] | None): The tokenized version of `text`. If not provided, it will be computed using\n            `tokenzier`. Default: `None`.\n        tokens_offsets (list[tuple[int, int] | None]): The offsets of `tokens` in `text`. If not provided, it will\n            be computed using `tokenzier`. Default: `None`.\n        edit_tokens (list[str] | None): The tokenized version of `edit`. If not provided, it will be computed using\n            `tokenzier`. Default: `None`.\n        edit_tokens_offsets (list[tuple[int, int] | None]): The offsets of `edit_tokens` in `edit`. If not\n            provided, it will be computed using `tokenzier`. Default: `None`.\n        aligned (WordOutput | None): The aligned `WordOutput` between `text` and `edit`. If not provided, it will\n            be obtained automatically using `tokenizer` for spltting. Default: `None`.\n        tokenizer (Tokenizer | None): A `Tokenizer` used for text splitting. If not provided, whitespace\n            tokenization is used by default. Default: `None`.\n        sub_label (str): The label for substitutions. Default: \"S\".\n        ins_label (str): The label for insertions. Default: \"I\".\n        del_label (str): The label for deletions. Default: \"D\".\n        gap_token (str): The token to use for gaps. Default: \"\u2581\".\n\n    Returns:\n        A tuple containing two lists of labels (one for `text`, one for `edit`). Each label can be either None\n            (if the token was not edited) or one of `sub_label`, `ins_label` or `del_label` depending on the type\n            of operation associated with the token.\n    \"\"\"\n    if tokenizer is None:\n        logger.info(\"Tokenizer was not provided. Defaulting to whitespace tokenization.\")\n        tokenizer = WhitespaceTokenizer()\n    if aligned is None:\n        aligned = process_words(\n            text, edit, texts_transform=tokenizer.transform, edits_transform=tokenizer.transform\n        )\n    tokens, tokens_offsets = cls._get_tokens_with_gaps(tokenizer, text, tokens, tokens_offsets, gap_token)\n    edit_tokens, edit_tokens_offsets = cls._get_tokens_with_gaps(\n        tokenizer, edit, edit_tokens, edit_tokens_offsets, gap_token\n    )\n    tokens_labels: list[str | None] = [None] * len(tokens)\n    edit_tokens_labels: list[str | None] = [None] * len(edit_tokens)\n    for alignment in aligned.alignments[0]:\n        text_start_idx = alignment.ref_start_idx\n        text_end_idx = alignment.ref_end_idx\n        edit_start_idx = alignment.hyp_start_idx\n        edit_end_idx = alignment.hyp_end_idx\n        if tokenizer.has_bos_token:\n            text_start_idx -= 1\n            text_end_idx -= 1\n            edit_start_idx -= 1\n            edit_end_idx -= 1\n        if alignment.type == \"insert\":\n            tokens_labels[text_start_idx * 2] = ins_label\n        elif alignment.type in (\"delete\", \"substitute\"):\n            label = sub_label if alignment.type == \"substitute\" else del_label\n            for idx in range(text_start_idx, text_end_idx):\n                tokens_labels[idx * 2 + 1] = label\n        if alignment.type == \"delete\":\n            edit_tokens_labels[edit_start_idx * 2] = del_label\n        elif alignment.type in (\"insert\", \"substitute\"):\n            label = sub_label if alignment.type == \"substitute\" else ins_label\n            for idx in range(edit_start_idx, edit_end_idx):\n                edit_tokens_labels[idx * 2 + 1] = label\n    return tokens_labels, edit_tokens_labels\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(text)","title":"<code>text</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(edit)","title":"<code>edit</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(tokens_offsets)","title":"<code>tokens_offsets</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(edit_tokens)","title":"<code>edit_tokens</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(edit_tokens_offsets)","title":"<code>edit_tokens_offsets</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(aligned)","title":"<code>aligned</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(sub_label)","title":"<code>sub_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(ins_label)","title":"<code>ins_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(del_label)","title":"<code>del_label</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens_labels_from_edit(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_tokens","title":"get_tokens","text":"<pre><code>get_tokens() -&gt; list[str]\n</code></pre> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def get_tokens(self) -&gt; list[str]:\n    return self.orig.tokens\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.get_labels","title":"get_labels","text":"<pre><code>get_labels() -&gt; Sequence[LabelType]\n</code></pre> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def get_labels(self) -&gt; Sequence[LabelType]:\n    return self.orig.tokens_labels\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; EditedEntryDictType\n</code></pre> <p>Convert the <code>EditedEntry</code> to a dictionary representation.</p> <p>Returns:</p> Type Description <code>EditedEntryDictType</code> <p>A dictionary representation of the <code>EditedEntry</code>.</p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def to_dict(self) -&gt; EditedEntryDictType:\n    \"\"\"Convert the `EditedEntry` to a dictionary representation.\n\n    Returns:\n        A dictionary representation of the `EditedEntry`.\n    \"\"\"\n    return EditedEntryDictType(\n        {\n            \"_class\": self.__class__.__name__,\n            \"info\": self.info,\n            \"orig\": self.orig.to_dict(),\n            \"edit\": self.edit.to_dict(),\n            \"has_bos_token\": self.has_bos_token,\n            \"has_eos_token\": self.has_eos_token,\n            \"has_gaps\": self.has_gaps,\n        }\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: EditedEntryDictType) -&gt; EditedEntry\n</code></pre> <p>Create a <code>EditedEntry</code> from a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>A dictionary representation of the <code>EditedEntry</code> obtained with <code>to_dict()</code>.</p> required <p>Returns:</p> Type Description <code>EditedEntry</code> <p>A <code>EditedEntry</code> object.</p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: EditedEntryDictType) -&gt; \"EditedEntry\":\n    \"\"\"Create a `EditedEntry` from a dictionary representation.\n\n    Args:\n        data (dict): A dictionary representation of the `EditedEntry` obtained with `to_dict()`.\n\n    Returns:\n        A `EditedEntry` object.\n    \"\"\"\n    if \"_class\" not in data:\n        raise RuntimeError(\"The provided dictionary is missing the required _class attribute.\")\n    if data[\"_class\"] != cls.__name__:\n        raise RuntimeError(f\"Cannot load a {cls.__name__} object from {data['_class']}\")\n    return cls(\n        orig=LabeledEntry.from_dict(data[\"orig\"]),\n        edit=LabeledEntry.from_dict(data[\"edit\"]),\n        has_bos_token=data[\"has_bos_token\"],\n        has_eos_token=data[\"has_eos_token\"],\n        has_gaps=data[\"has_gaps\"],\n        info=data[\"info\"],\n        constructor_key=cls.__constructor_key,\n    )\n</code></pre>"},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.from_dict(data)","title":"<code>data</code>","text":""},{"location":"api/data/entry/#labl.data.edited_entry.EditedEntry.merge_gap_annotations","title":"merge_gap_annotations","text":"<pre><code>merge_gap_annotations(\n    merge_fn: Callable[[Sequence[LabelType]], LabelType]\n    | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None\n</code></pre> <p>Merge gap annotations in the tokens of <code>orig</code> and <code>edit</code>.</p> <p>This method is equivalent to calling <code>EditedEntry.from_edits</code> with <code>with_gaps=False</code>. Gap annotations are merged to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last gap is kept to account for insertions at the end of the text.</p> <p>E.g. <code>GAP Hello GAP World GAP ! GAP</code> becomes <code>Hello World ! GAP</code>.      <code>I     S   I               I</code> <code>IS     I     I</code></p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def merge_gap_annotations(\n    self,\n    merge_fn: Callable[[Sequence[LabelType]], LabelType] | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None:\n    \"\"\"Merge gap annotations in the tokens of `orig` and `edit`.\n\n    This method is equivalent to calling `EditedEntry.from_edits` with `with_gaps=False`. Gap annotations are merged\n    to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last\n    gap is kept to account for insertions at the end of the text.\n\n    E.g. `GAP Hello GAP World GAP ! GAP` becomes `Hello World ! GAP`.\n         `  I     S   I               I`         `   IS     I     I`\n    \"\"\"\n    if not self._has_gaps:\n        raise RuntimeError(\"Gaps for the current entry were already merged.\")\n    has_bos = self._has_bos_token\n    if self.has_eos_token:\n        if not keep_final_gap:\n            raise RuntimeError(\n                \"The tokenizer has an EOS token, but `keep_final_gap` is set to False. The EOS token will be kept.\"\n            )\n    o_tok, o_lab, o_off = self._orig._tokens, self._orig._tokens_labels, self._orig._tokens_offsets\n    e_tok, e_lab, e_off = self._edit._tokens, self._edit._tokens_labels, self._edit._tokens_offsets\n    self._orig._tokens = Tokenizer._remove_gap_tokens([o_tok], self._has_bos_token, keep_final_gap)[0]\n    self._edit._tokens = Tokenizer._remove_gap_tokens([e_tok], self._has_bos_token, keep_final_gap)[0]\n    self._orig._tokens_labels = Tokenizer._merge_gap_annotations([o_lab], merge_fn, has_bos, keep_final_gap)[0]\n    self._edit._tokens_labels = Tokenizer._merge_gap_annotations([e_lab], merge_fn, has_bos, keep_final_gap)[0]\n    self._orig._tokens_offsets = Tokenizer._remove_gap_offsets([o_off], self._has_bos_token, keep_final_gap)[0]\n    self._edit._tokens_offsets = Tokenizer._remove_gap_offsets([e_off], self._has_bos_token, keep_final_gap)[0]\n    self._has_gaps = False\n    self._has_eos_token = keep_final_gap\n</code></pre>"},{"location":"api/data/multilabel/","title":"MultiLabelEntry","text":""},{"location":"api/data/multilabel/#labl.data.base_sequence.BaseMultiLabelEntry","title":"labl.data.base_sequence.BaseMultiLabelEntry","text":"<pre><code>BaseMultiLabelEntry(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseLabeledSequence[EntryType]</code>, <code>ABC</code></p> <p>Class for a list of <code>EntryType</code> objects representing multiple labels over the same text.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/multilabel/#labl.data.base_sequence.BaseMultiLabelEntry.label_counts","title":"label_counts  <code>property</code>","text":"<pre><code>label_counts: list[int]\n</code></pre> <p>Counts the number of labels for each token in the original text.</p>"},{"location":"api/data/multilabel/#labl.data.base_sequence.BaseMultiLabelEntry.tokens_with_label_counts","title":"tokens_with_label_counts  <code>property</code>","text":"<pre><code>tokens_with_label_counts: LabeledTokenList\n</code></pre> <p>Returns a list of <code>LabeledToken</code> with the number of labels for each token in the original text.</p>"},{"location":"api/data/multilabel/#labl.data.base_sequence.BaseMultiLabelEntry.get_agreement","title":"get_agreement","text":"<pre><code>get_agreement(\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput\n</code></pre> <p>Compute the inter-annotator agreement for the token labels of all label sets using Krippendorff's alpha.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def get_agreement(\n    self,\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput:\n    \"\"\"Compute the inter-annotator agreement for the token labels of all label sets using\n    [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha).\n    \"\"\"\n    self._validate_single_label_type()\n    labels_array = self._get_labels_array(dtype=self.label_types[0])\n    return get_labels_agreement(\n        label_type=self.label_types[0],\n        labels_array=labels_array,\n        level_of_measurement=level_of_measurement,\n    )\n</code></pre>"},{"location":"api/data/multilabel/#labl.data.labeled_entry.MultiLabelEntry","title":"labl.data.labeled_entry.MultiLabelEntry","text":"<pre><code>MultiLabelEntry(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseMultiLabelEntry[LabeledEntry]</code></p> <p>Class for a list of <code>LabeledEntry</code> representing multiple labels over the same text.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/multilabel/#labl.data.edited_entry.MultiEditEntry","title":"labl.data.edited_entry.MultiEditEntry","text":"<pre><code>MultiEditEntry(\n    iterable: Iterable[LabeledObject] | None = None,\n    *,\n    info: InfoDictType = {},\n)\n</code></pre> <p>               Bases: <code>BaseMultiLabelEntry[EditedEntry]</code></p> <p>Class for a list of <code>EditedEntry</code> representing multiple edits over the same <code>orig</code> text.</p> Source code in <code>labl/data/base_sequence.py</code> <pre><code>def __init__(self, iterable: Iterable[LabeledObject] | None = None, *, info: InfoDictType = {}):\n    if iterable is None:\n        super().__init__()\n    else:\n        super().__init__(iterable)\n    self._label_types = self._get_label_types()\n    self._info = info\n</code></pre>"},{"location":"api/data/multilabel/#labl.data.edited_entry.MultiEditEntry.merge_gap_annotations","title":"merge_gap_annotations","text":"<pre><code>merge_gap_annotations(\n    merge_fn: Callable[[Sequence[LabelType]], LabelType]\n    | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None\n</code></pre> <p>Merge gap annotations in the tokens of <code>orig</code> and <code>edit</code>.</p> <p>This method is equivalent to calling <code>EditedEntry.from_edits</code> with <code>with_gaps=False</code>. Gap annotations are merged to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last gap is kept to account for insertions at the end of the text.</p> <p>E.g. <code>GAP Hello GAP World GAP ! GAP</code> becomes <code>Hello World ! GAP</code>.      <code>I     S   I               I</code> <code>IS     I     I</code></p> Source code in <code>labl/data/edited_entry.py</code> <pre><code>def merge_gap_annotations(\n    self,\n    merge_fn: Callable[[Sequence[LabelType]], LabelType] | None = None,\n    keep_final_gap: bool = True,\n) -&gt; None:\n    \"\"\"Merge gap annotations in the tokens of `orig` and `edit`.\n\n    This method is equivalent to calling `EditedEntry.from_edits` with `with_gaps=False`. Gap annotations are merged\n    to the next non-gap token to the right, and the gap label is added to the label of the non-gap token. The last\n    gap is kept to account for insertions at the end of the text.\n\n    E.g. `GAP Hello GAP World GAP ! GAP` becomes `Hello World ! GAP`.\n         `  I     S   I               I`         `   IS     I     I`\n    \"\"\"\n    for entry in self:\n        entry.merge_gap_annotations(merge_fn=merge_fn, keep_final_gap=keep_final_gap)\n</code></pre>"},{"location":"api/datasets/translation/","title":"Translation","text":""},{"location":"api/datasets/translation/#labl.datasets.translation.load_qe4pe","title":"labl.datasets.translation.load_qe4pe","text":"<pre><code>load_qe4pe(\n    configs: Qe4peTask | list[Qe4peTask] = \"main\",\n    langs: Qe4peLanguage | list[Qe4peLanguage] = [\n        \"ita\",\n        \"nld\",\n    ],\n    domains: Qe4peDomain | list[Qe4peDomain] | None = None,\n    speed_groups: Qe4peSpeedGroup\n    | list[Qe4peSpeedGroup]\n    | None = None,\n    highlight_modalities: Qe4peHighlightModality\n    | list[Qe4peHighlightModality]\n    | None = None,\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    filter_issues: bool = True,\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; dict[str, dict[str, EditedDataset]]\n</code></pre> <p>Load the QE4PE dataset by Sarti et al. (2025), containing multiple edits     over a single set of machine-translated sentences in two languages (Italian and Dutch).</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Literal[\"pretask\", \"main\", \"posttask\"] | list[Literal[\"pretask\", \"main\", \"posttask\"]], *optional*</code> <p>One or more task configurations to load. Defaults to \"main\". Available options: \"pretask\", \"main\", \"posttask\".</p> <code>'main'</code> <code>langs</code> <code>Literal[\"ita\", \"nld\"] | list[Literal[\"ita\", \"nld\"]], *optional*</code> <p>One or more languages to load. Defaults to [\"ita\", \"nld\"]. Available options: \"ita\", \"nld\".</p> <code>['ita', 'nld']</code> <code>domains</code> <code>Literal[\"biomedical\", \"social\"] | list[Literal[\"biomedical\", \"social\"]] | None, *optional*</code> <p>One or more text categories to load. Defaults to [\"biomedical\", \"social\"]. Available options: \"biomedical\", \"social\".</p> <code>None</code> <code>speed_groups</code> <code>Literal[\"faster\", \"avg\", \"slower\"] | list[Literal[\"faster\", \"avg\", \"slower\"]] | None, *optional*</code> <p>One or more translator speed groups to load. Defaults to [\"faster\", \"avg\", \"slower\"]. Available options: \"faster\", \"avg\", \"slower\".</p> <code>None</code> <code>highlight_modalities</code> <code>Literal[\"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\"] | list[Literal[\"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\"]] | None, *optional*</code> <p>One or more highlight modalities to load. Defaults to all modalities. Available options: \"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\".</p> <code>None</code> <code>filter_issues</code> <code>bool, *optional*</code> <p>Whether to filter out issues from the dataset. Defaults to True.</p> <code>True</code> <code>tokenizer</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*</code> <p>The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.</p> <code>None</code> <code>tokenizer_kwargs</code> <code>dict[str, Any], *optional*</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>with_gaps</code> <code>bool, *optional*</code> <p>Whether to include gaps in the tokenization. Defaults to True.</p> <code>True</code> <code>sub_label</code> <code>str, *optional*</code> <p>The label for substitutions. Defaults to \"S\".</p> <code>'S'</code> <code>ins_label</code> <code>str, *optional*</code> <p>The label for insertions. Defaults to \"I\".</p> <code>'I'</code> <code>del_label</code> <code>str, *optional*</code> <p>The label for deletions. Defaults to \"D\".</p> <code>'D'</code> <code>gap_token</code> <code>str, *optional*</code> <p>The token used for gaps. Defaults to \"\u2581\".</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, EditedDataset]]</code> <p>A dictionary containing the loaded datasets for each task and language. The keys are the task configurations, and the values are dictionaries with language keys and <code>EditedDataset</code> objects as values. E.g. <code>load_qe4pe_dataset()[\"main\"][\"ita\"]</code> returns the <code>EditedDataset</code> for the main task for Italian.</p> Source code in <code>labl/datasets/translation/qe4pe.py</code> <pre><code>def load_qe4pe(\n    configs: Qe4peTask | list[Qe4peTask] = \"main\",\n    langs: Qe4peLanguage | list[Qe4peLanguage] = [\"ita\", \"nld\"],\n    domains: Qe4peDomain | list[Qe4peDomain] | None = None,\n    speed_groups: Qe4peSpeedGroup | list[Qe4peSpeedGroup] | None = None,\n    highlight_modalities: Qe4peHighlightModality | list[Qe4peHighlightModality] | None = None,\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    filter_issues: bool = True,\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; dict[str, dict[str, EditedDataset]]:\n    \"\"\"Load the QE4PE dataset by [Sarti et al. (2025)](https://arxiv.org/abs/2503.03044), containing multiple edits\n        over a single set of machine-translated sentences in two languages (Italian and Dutch).\n\n    Args:\n        configs (Literal[\"pretask\", \"main\", \"posttask\"] | list[Literal[\"pretask\", \"main\", \"posttask\"]], *optional*):\n            One or more task configurations to load. Defaults to \"main\".\n            Available options: \"pretask\", \"main\", \"posttask\".\n        langs (Literal[\"ita\", \"nld\"] | list[Literal[\"ita\", \"nld\"]], *optional*):\n            One or more languages to load. Defaults to [\"ita\", \"nld\"].\n            Available options: \"ita\", \"nld\".\n        domains (Literal[\"biomedical\", \"social\"] | list[Literal[\"biomedical\", \"social\"]] | None, *optional*):\n            One or more text categories to load. Defaults to [\"biomedical\", \"social\"].\n            Available options: \"biomedical\", \"social\".\n        speed_groups (Literal[\"faster\", \"avg\", \"slower\"] | list[Literal[\"faster\", \"avg\", \"slower\"]] | None, *optional*):\n            One or more translator speed groups to load. Defaults to [\"faster\", \"avg\", \"slower\"].\n            Available options: \"faster\", \"avg\", \"slower\".\n        highlight_modalities (Literal[\"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\"] | list[Literal[\"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\"]] | None, *optional*):\n            One or more highlight modalities to load. Defaults to all modalities.\n            Available options: \"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\".\n        filter_issues (bool, *optional*):\n            Whether to filter out issues from the dataset. Defaults to True.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*):\n            The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.\n        tokenizer_kwargs (dict[str, Any], *optional*):\n            Additional arguments for the tokenizer.\n        with_gaps (bool, *optional*):\n            Whether to include gaps in the tokenization. Defaults to True.\n        sub_label (str, *optional*):\n            The label for substitutions. Defaults to \"S\".\n        ins_label (str, *optional*):\n            The label for insertions. Defaults to \"I\".\n        del_label (str, *optional*):\n            The label for deletions. Defaults to \"D\".\n        gap_token (str, *optional*):\n            The token used for gaps. Defaults to \"\u2581\".\n\n    Returns:\n        A dictionary containing the loaded datasets for each task and language.\n            The keys are the task configurations, and the values are dictionaries with language keys\n            and `EditedDataset` objects as values. E.g. `load_qe4pe_dataset()[\"main\"][\"ita\"]` returns the\n            `EditedDataset` for the main task for Italian.\n    \"\"\"\n    if not is_datasets_available() or not is_pandas_available():\n        raise RuntimeError(\"The `datasets` library is not installed. Please install it to use this function.\")\n    import pandas as pd\n\n    from datasets import DatasetDict, load_dataset\n\n    if isinstance(configs, str):\n        configs = [configs]\n    if isinstance(langs, str):\n        langs = [langs]\n    if domains is None:\n        domains = [\"biomedical\", \"social\"]\n    if isinstance(domains, str):\n        domains = [domains]\n    if speed_groups is None:\n        speed_groups = [\"faster\", \"avg\", \"slower\"]\n    if isinstance(speed_groups, str):\n        speed_groups = [speed_groups]\n    if highlight_modalities is None:\n        highlight_modalities = [\"no_highlight\", \"oracle\", \"supervised\", \"unsupervised\"]\n    if isinstance(highlight_modalities, str):\n        highlight_modalities = [highlight_modalities]\n    out_dict = {}\n    for config in configs:\n        dataset = cast(DatasetDict, load_dataset(\"gsarti/qe4pe\", config))\n        df = cast(pd.DataFrame, dataset[\"train\"].to_pandas())\n        if filter_issues:\n            df = df[(~df[\"has_issue\"]) &amp; (df[\"translator_main_id\"] != \"no_highlight_t4\")]\n        out_dict[config] = {}\n        for lang in langs:\n            print(f\"Loading {config} task for eng-&gt;{lang}...\")\n            lang_df = df[(df[\"tgt_lang\"] == lang) &amp; df[\"wmt_category\"].isin(domains)]\n            lang_df = lang_df[\n                lang_df[\"translator_main_id\"].str.endswith(tuple(SPEED_MAP[g] for g in speed_groups))\n                | lang_df[\"highlight_modality\"].isin(highlight_modalities)\n            ]\n            labl_dataset = EditedDataset.from_edits_dataframe(\n                lang_df,\n                text_column=\"mt_text\",\n                edit_column=\"pe_text\",\n                entry_ids=[\"doc_id\", \"segment_in_doc_id\"],\n                infos_columns=[\n                    \"wmt_category\",\n                    \"doc_id\",\n                    \"segment_in_doc_id\",\n                    \"translator_main_id\",\n                    \"highlight_modality\",\n                ],\n                tokenizer=tokenizer,\n                tokenizer_kwargs=tokenizer_kwargs,\n                with_gaps=with_gaps,\n                sub_label=sub_label,\n                ins_label=ins_label,\n                del_label=del_label,\n                gap_token=gap_token,\n            )\n            out_dict[config][lang] = labl_dataset\n    return out_dict\n</code></pre>"},{"location":"api/datasets/translation/#labl.datasets.translation.load_divemt","title":"labl.datasets.translation.load_divemt","text":"<pre><code>load_divemt(\n    configs: DivemtTask | list[DivemtTask] = \"main\",\n    langs: DivemtLanguage | list[DivemtLanguage] = [\n        \"ara\",\n        \"nld\",\n        \"ita\",\n        \"tur\",\n        \"ukr\",\n        \"vie\",\n    ],\n    mt_models: DivemtMTModel | list[DivemtMTModel] = [\n        \"gtrans\",\n        \"mbart50\",\n    ],\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; dict[str, dict[str, dict[str, EditedDataset]]]\n</code></pre> <p>Load the DivEMT dataset by Sarti et al. (2022), containing edits     over two sets of machine-translated sentences across six typologically diverse languages.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Literal[\"warmup\", \"main\"] | list[Literal[\"warmup\", \"main\"]], *optional*</code> <p>One or more task configurations to load. Defaults to \"main\". Available options: \"warmup\", \"main\".</p> <code>'main'</code> <code>langs</code> <code>Literal[\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"] | list[Literal[\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"]], *optional*</code> <p>One or more languages to load. Defaults to [\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"]. Available options: \"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\".</p> <code>['ara', 'nld', 'ita', 'tur', 'ukr', 'vie']</code> <code>mt_models</code> <code>Literal[\"gtrans\", \"mbart50\"] | list[Literal[\"gtrans\", \"mbart50\"]], *optional*</code> <p>One or more models for which post-edits need to be loaded. Defaults to [\"gtrans\", \"mbart50\"]. Available options: \"gtrans\", \"mbart50\".</p> <code>['gtrans', 'mbart50']</code> <code>tokenizer</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*</code> <p>The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.</p> <code>None</code> <code>tokenizer_kwargs</code> <code>dict[str, Any], *optional*</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <code>with_gaps</code> <code>bool, *optional*</code> <p>Whether to include gaps in the tokenization. Defaults to True.</p> <code>True</code> <code>sub_label</code> <code>str, *optional*</code> <p>The label for substitutions. Defaults to \"S\".</p> <code>'S'</code> <code>ins_label</code> <code>str, *optional*</code> <p>The label for insertions. Defaults to \"I\".</p> <code>'I'</code> <code>del_label</code> <code>str, *optional*</code> <p>The label for deletions. Defaults to \"D\".</p> <code>'D'</code> <code>gap_token</code> <code>str, *optional*</code> <p>The token used for gaps. Defaults to \"\u2581\".</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, dict[str, EditedDataset]]]</code> <p>A dictionary containing the loaded datasets for each task, language, and MT model. The keys are the task configurations, and the values are dictionaries with language keys and <code>EditedDataset</code> objects as values. E.g. <code>load_divemt_dataset()[\"main\"][\"ita\"][\"mbart50\"]</code> returns the <code>EditedDataset</code> for the main task for Italian.</p> Source code in <code>labl/datasets/translation/divemt.py</code> <pre><code>def load_divemt(\n    configs: DivemtTask | list[DivemtTask] = \"main\",\n    langs: DivemtLanguage | list[DivemtLanguage] = [\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"],\n    mt_models: DivemtMTModel | list[DivemtMTModel] = [\"gtrans\", \"mbart50\"],\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n    with_gaps: bool = True,\n    sub_label: str = \"S\",\n    ins_label: str = \"I\",\n    del_label: str = \"D\",\n    gap_token: str = \"\u2581\",\n) -&gt; dict[str, dict[str, dict[str, EditedDataset]]]:\n    \"\"\"Load the DivEMT dataset by [Sarti et al. (2022)](https://aclanthology.org/2022.emnlp-main.532/), containing edits\n        over two sets of machine-translated sentences across six typologically diverse languages.\n\n    Args:\n        configs (Literal[\"warmup\", \"main\"] | list[Literal[\"warmup\", \"main\"]], *optional*):\n            One or more task configurations to load. Defaults to \"main\".\n            Available options: \"warmup\", \"main\".\n        langs (Literal[\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"] | list[Literal[\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"]], *optional*):\n            One or more languages to load. Defaults to [\"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\"].\n            Available options: \"ara\", \"nld\", \"ita\", \"tur\", \"ukr\", \"vie\".\n        mt_models (Literal[\"gtrans\", \"mbart50\"] | list[Literal[\"gtrans\", \"mbart50\"]], *optional*):\n            One or more models for which post-edits need to be loaded. Defaults to [\"gtrans\", \"mbart50\"].\n            Available options: \"gtrans\", \"mbart50\".\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*):\n            The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.\n        tokenizer_kwargs (dict[str, Any], *optional*):\n            Additional arguments for the tokenizer.\n        with_gaps (bool, *optional*):\n            Whether to include gaps in the tokenization. Defaults to True.\n        sub_label (str, *optional*):\n            The label for substitutions. Defaults to \"S\".\n        ins_label (str, *optional*):\n            The label for insertions. Defaults to \"I\".\n        del_label (str, *optional*):\n            The label for deletions. Defaults to \"D\".\n        gap_token (str, *optional*):\n            The token used for gaps. Defaults to \"\u2581\".\n\n    Returns:\n        A dictionary containing the loaded datasets for each task, language, and MT model.\n            The keys are the task configurations, and the values are dictionaries with language keys\n            and `EditedDataset` objects as values. E.g. `load_divemt_dataset()[\"main\"][\"ita\"][\"mbart50\"]` returns the\n            `EditedDataset` for the main task for Italian.\n    \"\"\"\n    if not is_datasets_available() or not is_pandas_available():\n        raise RuntimeError(\"The `datasets` library is not installed. Please install it to use this function.\")\n    import pandas as pd\n\n    from datasets import DatasetDict, load_dataset\n\n    if isinstance(configs, str):\n        configs = [configs]\n    if isinstance(langs, str):\n        langs = [langs]\n    if isinstance(mt_models, str):\n        mt_models = [mt_models]\n    out_dict = {}\n    for config in configs:\n        dataset = cast(DatasetDict, load_dataset(\"GroNLP/divemt\", config))\n        df = cast(pd.DataFrame, dataset[\"train\"].to_pandas())\n        out_dict[config] = {}\n        for lang in langs:\n            out_dict[config][lang] = {}\n            for model in mt_models:\n                print(f\"Loading {config} task for eng-&gt;{lang} {model} edits...\")\n                filter_df = df[(df[\"lang_id\"] == lang) &amp; (df[\"task_type\"] == MT_MODEL_MAP[model])]\n                labl_dataset = EditedDataset.from_edits_dataframe(\n                    filter_df,\n                    text_column=\"mt_text\",\n                    edit_column=\"tgt_text\",\n                    entry_ids=\"item_id\",\n                    infos_columns=[\"doc_id\", \"subject_id\", \"item_id\"],\n                    tokenizer=tokenizer,\n                    tokenizer_kwargs=tokenizer_kwargs,\n                    with_gaps=with_gaps,\n                    sub_label=sub_label,\n                    ins_label=ins_label,\n                    del_label=del_label,\n                    gap_token=gap_token,\n                )\n                out_dict[config][lang][model] = labl_dataset\n    return out_dict\n</code></pre>"},{"location":"api/datasets/translation/#labl.datasets.translation.load_wmt24esa","title":"labl.datasets.translation.load_wmt24esa","text":"<pre><code>load_wmt24esa(\n    langs: Wmt24EsaLanguage\n    | list[Wmt24EsaLanguage]\n    | None = None,\n    domains: Wmt24EsaDomain\n    | list[Wmt24EsaDomain]\n    | None = None,\n    mt_models: Wmt24EsaMTModel\n    | list[Wmt24EsaMTModel]\n    | None = None,\n    tokenizer: str\n    | Tokenizer\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast\n    | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n) -&gt; dict[str, dict[str, LabeledDataset]]\n</code></pre> <p>Load the WMT24 ESA annotations from Kocmi et al. (2024), containing     partially overlapping segments across multiple language pairs with a single set of     ESA annotations over multiple MT system outputs.</p> <p>Parameters:</p> Name Type Description Default <code>langs</code> <code>Wmt24EsaLanguage | list[Wmt24EsaLanguage] | None</code> <p>One or more languages to load. Defaults to <code>[\"en-cs\", \"en-ja\", \"en-es\", \"en-zh\", \"en-hi\", \"en-is\", \"cs-uk\",     \"en-uk\", \"en-ru\"]. Available options:</code>\"en-cs\", \"en-ja\", \"en-es\", \"en-zh\", \"en-hi\", \"en-is\", \"cs-uk\",     \"en-uk\", \"en-ru\"`.</p> <code>None</code> <code>domains</code> <code>Wmt24EsaDomain | list[Wmt24EsaDomain] | None</code> <p>One or more text categories to load. Defaults to <code>[\"speech\", \"social\", \"news\", \"literary\", \"education\",     \"voice\", \"personal\", \"official\"]</code>. Available options: <code>\"speech\", \"social\", \"news\", \"literary\",     \"education\", \"voice\", \"personal\", \"official\"</code>.</p> <code>None</code> <code>mt_models</code> <code>Wmt24EsaMTModel | list[Wmt24EsaMTModel] | None</code> <p>One or more models for which annotations need to be loaded. Defaults to all models. Available options: <code>\"Unbabel-Tower70B\", \"CUNI-GA\", \"Gemini-1.5-Pro\", \"SCIR-MT\", \"Aya23\", \"Claude-3.5\",     \"ONLINE-W\", \"Llama3-70B\", \"GPT-4\", \"CommandR-plus\", \"IKUN-C\", \"refA\", \"IOL-Research\",     \"CUNI-DocTransformer\", \"IKUN\", \"CUNI-MH\", \"Mistral-Large\", \"ONLINE-B\", \"Dubformer\", \"MSLC\",     \"Team-J\", \"HW-TSC\", \"NTTSU\", \"TranssionMT\", \"AMI\", \"CUNI-Transformer\", \"ONLINE-G\", \"Yandex\"</code>.</p> <code>None</code> <code>tokenizer</code> <code>str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*</code> <p>The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.</p> <code>None</code> <code>tokenizer_kwargs</code> <code>dict[str, Any], *optional*</code> <p>Additional arguments for the tokenizer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, LabeledDataset]]</code> <p>A dictionary containing the loaded datasets for each MT model and language. The keys are the task configurations, and the values are dictionaries with language keys and <code>EditedDataset</code> objects as values. E.g. <code>load_wmt24esa()[\"Aya23\"][\"en-cs\"]</code> returns the <code>LabeledDataset</code> for the Aya23 model for English-Czech.</p> Source code in <code>labl/datasets/translation/wmt24_esa.py</code> <pre><code>def load_wmt24esa(\n    langs: Wmt24EsaLanguage | list[Wmt24EsaLanguage] | None = None,\n    domains: Wmt24EsaDomain | list[Wmt24EsaDomain] | None = None,\n    mt_models: Wmt24EsaMTModel | list[Wmt24EsaMTModel] | None = None,\n    tokenizer: str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast | None = None,\n    tokenizer_kwargs: dict[str, Any] = {},\n) -&gt; dict[str, dict[str, LabeledDataset]]:\n    \"\"\"Load the WMT24 ESA annotations from [Kocmi et al. (2024)](https://aclanthology.org/2024.wmt-1.1/), containing\n        partially overlapping segments across multiple language pairs with a single set of\n        [ESA annotations](https://aclanthology.org/2024.wmt-1.131/) over multiple MT system outputs.\n\n    Args:\n        langs (Wmt24EsaLanguage | list[Wmt24EsaLanguage] | None):\n            One or more languages to load. Defaults to `[\"en-cs\", \"en-ja\", \"en-es\", \"en-zh\", \"en-hi\", \"en-is\", \"cs-uk\",\n                \"en-uk\", \"en-ru\"]. Available options: `\"en-cs\", \"en-ja\", \"en-es\", \"en-zh\", \"en-hi\", \"en-is\", \"cs-uk\",\n                \"en-uk\", \"en-ru\"`.\n        domains (Wmt24EsaDomain | list[Wmt24EsaDomain] | None):\n            One or more text categories to load. Defaults to `[\"speech\", \"social\", \"news\", \"literary\", \"education\",\n                \"voice\", \"personal\", \"official\"]`. Available options: `\"speech\", \"social\", \"news\", \"literary\",\n                \"education\", \"voice\", \"personal\", \"official\"`.\n        mt_models (Wmt24EsaMTModel | list[Wmt24EsaMTModel] | None):\n            One or more models for which annotations need to be loaded. Defaults to all models.\n            Available options: `\"Unbabel-Tower70B\", \"CUNI-GA\", \"Gemini-1.5-Pro\", \"SCIR-MT\", \"Aya23\", \"Claude-3.5\",\n                \"ONLINE-W\", \"Llama3-70B\", \"GPT-4\", \"CommandR-plus\", \"IKUN-C\", \"refA\", \"IOL-Research\",\n                \"CUNI-DocTransformer\", \"IKUN\", \"CUNI-MH\", \"Mistral-Large\", \"ONLINE-B\", \"Dubformer\", \"MSLC\",\n                \"Team-J\", \"HW-TSC\", \"NTTSU\", \"TranssionMT\", \"AMI\", \"CUNI-Transformer\", \"ONLINE-G\", \"Yandex\"`.\n        tokenizer (str | Tokenizer | PreTrainedTokenizer | PreTrainedTokenizerFast, *optional*):\n            The tokenizer to use for tokenization. If None, a default whitespace tokenizer will be used.\n        tokenizer_kwargs (dict[str, Any], *optional*):\n            Additional arguments for the tokenizer.\n\n    Returns:\n        A dictionary containing the loaded datasets for each MT model and language.\n            The keys are the task configurations, and the values are dictionaries with language keys\n            and `EditedDataset` objects as values. E.g. `load_wmt24esa()[\"Aya23\"][\"en-cs\"]` returns\n            the `LabeledDataset` for the Aya23 model for English-Czech.\n    \"\"\"\n    if not is_pandas_available():\n        raise RuntimeError(\"The `pandas` library is not installed. Please install it to use this function.\")\n\n    if langs is None:\n        langs = [\"en-cs\", \"en-ja\", \"en-es\", \"en-zh\", \"en-hi\", \"en-is\", \"cs-uk\", \"en-uk\", \"en-ru\"]\n    if isinstance(langs, str):\n        langs = [langs]\n    if domains is None:\n        domains = [\"speech\", \"social\", \"news\", \"literary\", \"education\", \"voice\", \"personal\", \"official\"]\n    if isinstance(domains, str):\n        domains = [domains]\n    if mt_models is None:\n        mt_models = [\n            \"Unbabel-Tower70B\",\n            \"CUNI-GA\",\n            \"Gemini-1.5-Pro\",\n            \"SCIR-MT\",\n            \"Aya23\",\n            \"Claude-3.5\",\n            \"ONLINE-W\",\n            \"Llama3-70B\",\n            \"GPT-4\",\n            \"CommandR-plus\",\n            \"IKUN-C\",\n            \"refA\",\n            \"IOL-Research\",\n            \"CUNI-DocTransformer\",\n            \"IKUN\",\n            \"CUNI-MH\",\n            \"Mistral-Large\",\n            \"ONLINE-B\",\n            \"Dubformer\",\n            \"MSLC\",\n            \"Team-J\",\n            \"HW-TSC\",\n            \"NTTSU\",\n            \"TranssionMT\",\n            \"AMI\",\n            \"CUNI-Transformer\",\n            \"ONLINE-G\",\n            \"Yandex\",\n        ]\n    if isinstance(mt_models, str):\n        mt_models = [mt_models]\n    out_dict = {}\n    df = load_cached_or_download(\n        url=\"https://raw.githubusercontent.com/wmt-conference/wmt24-news-systems/refs/heads/main/jsonl/wmt24_esa.jsonl\",\n        filetype=\"jsonl\",\n    )\n    for model in mt_models:\n        out_dict[model] = {}\n        for lang in langs:\n            filter_df = df[df[\"domain\"].isin(domains) &amp; (df[\"system\"] == model) &amp; (df[\"langs\"] == lang)]\n            all_spans = []\n            all_infos = []\n            if not filter_df.empty:\n                print(f\"Loading {model} annotations for {lang}...\")\n                for _, row in filter_df.iterrows():\n                    spans = []\n                    infos = {c: row[c] for c in [\"line_id\", \"doc_id\", \"domain\", \"esa_score\", \"annotator\"]}\n                    for span in row[\"esa_spans\"]:\n                        start, end = span[\"start_i\"], span[\"end_i\"]\n                        if isinstance(start, int) and isinstance(end, int) and start &lt; end:\n                            spans.append(\n                                Span(\n                                    start=span[\"start_i\"],\n                                    end=span[\"end_i\"],\n                                    label=span[\"severity\"],\n                                    text=row[\"tgt\"][start:end],\n                                )\n                            )\n                    all_spans.append(spans)\n                    all_infos.append(infos)\n                labl_dataset = LabeledDataset.from_spans(\n                    texts=list(filter_df[\"tgt\"]),\n                    spans=all_spans,\n                    infos=all_infos,\n                    tokenizer=tokenizer,\n                    tokenizer_kwargs=tokenizer_kwargs,\n                )\n                out_dict[model][lang] = labl_dataset\n    return out_dict\n</code></pre>"},{"location":"api/utils/agreement/","title":"Agreement","text":""},{"location":"api/utils/agreement/#labl.utils.agreement.AgreementOutput","title":"labl.utils.agreement.AgreementOutput  <code>dataclass</code>","text":"<pre><code>AgreementOutput(\n    full: float | None,\n    pair: list[list[float]],\n    type: MeasurementType,\n)\n</code></pre> <p>Data class for storing the output of the inter-annotator agreement computation.</p> <p>Attributes:</p> Name Type Description <code>full</code> <code>float | None</code> <p>The full agreement for all annotation sets.</p> <code>pair</code> <code>list[list[float]]</code> <p>Pairwise agreement between all annotators.</p> <code>type</code> <code>str</code> <p>The type of agreement measure employed.</p>"},{"location":"api/utils/agreement/#labl.utils.agreement.get_labels_agreement","title":"labl.utils.agreement.get_labels_agreement","text":"<pre><code>get_labels_agreement(\n    label_type: type,\n    labels_array: NDArray[ValueScalarType],\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput\n</code></pre> <p>Compute the inter-annotator agreement using Krippendorff's alpha for an (M, N) array of labels, where M is the number of annotators and N is the number of units.</p> <p>Parameters:</p> Name Type Description Default <code>Literal['nominal', 'ordinal', 'interval', 'ratio']</code> <p>The level of measurement for the labels when using Krippendorff's alpha. Can be \"nominal\", \"ordinal\", \"interval\", or \"ratio\", depending on the type of labels. Default: \"nominal\" for string labels, \"ordinal\" for int labels, and \"interval\" for float labels.</p> <code>None</code> <p>Returns:</p> Type Description <code>AgreementOutput</code> <p>Labels correlation (for numeric) or inter-annotator agreement (for categorical) between the two entries</p> Source code in <code>labl/utils/agreement.py</code> <pre><code>def get_labels_agreement(\n    label_type: type,\n    labels_array: npt.NDArray[ValueScalarType],\n    level_of_measurement: LevelOfMeasurement | None = None,\n) -&gt; AgreementOutput:\n    \"\"\"Compute the inter-annotator agreement using\n    [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha) for an (M, N) array of labels,\n    where M is the number of annotators and N is the number of units.\n\n    Args:\n        level_of_measurement (Literal['nominal', 'ordinal', 'interval', 'ratio']): The level of measurement for the\n            labels when using Krippendorff's alpha. Can be \"nominal\", \"ordinal\", \"interval\", or \"ratio\", depending\n            on the type of labels. Default: \"nominal\" for string labels, \"ordinal\" for int labels, and \"interval\"\n            for float labels.\n\n    Returns:\n        Labels correlation (for numeric) or inter-annotator agreement (for categorical) between the two entries\n    \"\"\"\n    num_annotators = labels_array.shape[0]\n    if level_of_measurement is None:\n        if label_type is str:\n            level_of_measurement = \"nominal\"\n        elif label_type is int:\n            level_of_measurement = \"ordinal\"\n        elif label_type is float:\n            level_of_measurement = \"interval\"\n        else:\n            raise ValueError(\n                f\"Unsupported label type: {label_type}. Please specify the level of measurement explicitly.\"\n            )\n    if labels_array.dtype.kind in {\"i\", \"u\", \"f\"}:\n        unique_vals = np.unique(labels_array[~np.isnan(labels_array)])\n    elif labels_array.dtype.kind in {\"U\", \"S\"}:  # Unicode or byte string.\n        # `np.asarray` will coerce `np.nan` values to \"nan\".\n        unique_vals = np.unique(labels_array[labels_array != \"nan\"])\n    else:\n        raise ValueError(\n            f\"Unsupported label type: {labels_array.dtype}. Please specify the level of measurement explicitly.\"\n        )\n    measurement_type = None\n    if len(unique_vals) &gt; 1:\n        full_score = alpha(reliability_data=labels_array, level_of_measurement=level_of_measurement)\n        pair_scores = np.identity(num_annotators)\n        for i in range(num_annotators):\n            for j in range(i + 1, num_annotators):\n                if np.array_equal(\n                    labels_array[i, :], labels_array[j, :], equal_nan=True if label_type is float else False\n                ):\n                    pair_score = 1.0\n                else:\n                    pair_score = alpha(\n                        reliability_data=labels_array[[i, j], :],\n                        level_of_measurement=level_of_measurement,\n                    )\n                pair_scores[i, j] = pair_score\n                pair_scores[j, i] = pair_score\n        measurement_type = \"krippendorff_\" + (\n            level_of_measurement if isinstance(level_of_measurement, str) else \"custom\"\n        )\n    else:\n        full_score = None\n        binary_labels = (labels_array == unique_vals[0]).astype(int)\n        pair_scores = spearmanr(binary_labels, axis=1).statistic  # type: ignore\n        measurement_type = \"spearmanr_binary\"\n    pair_scores = cast(list[list[float]], pair_scores.tolist())\n    return AgreementOutput(\n        full=full_score,\n        pair=pair_scores,\n        type=measurement_type,\n    )\n</code></pre>"},{"location":"api/utils/agreement/#labl.utils.agreement.get_labels_agreement(level_of_measurement)","title":"<code>level_of_measurement</code>","text":""},{"location":"api/utils/tokenizer/","title":"Tokenizer","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer","title":"labl.utils.tokenizer.Tokenizer","text":"<pre><code>Tokenizer(\n    transform: AbstractTransform | Compose,\n    has_bos_token: bool = False,\n    has_eos_token: bool = False,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for tokenizers.</p> <p>This class provides a common interface for tokenizing and detokenizing text, unifying the behavior of <code>jiwer</code> and <code>transformers</code> tokenizers for alignment and visualization.</p> <p>Attributes:</p> Name Type Description <code>transform</code> <code>AbstractTransform | Compose</code> <p>The transformation to apply to the input strings. This should be a composition of transformations that includes a final step producing a list of list of tokens, following jiwer transformations.</p> <code>has_bos_token</code> <code>bool</code> <p>Whether the tokenizer sets a beginning-of-sequence token. Defaults to False.</p> <code>has_eos_token</code> <code>bool</code> <p>Whether the tokenizer sets an end-of-sequence token. Defaults to False.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.__call__","title":"__call__","text":"<pre><code>__call__(\n    texts: str | list[str], with_offsets: bool = False\n) -&gt; (\n    list[list[str]]\n    | tuple[list[list[str]], Sequence[Sequence[OffsetType]]]\n)\n</code></pre> <p>Tokenizes one or more input strings.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The strings to tokenize.</p> required <code>bool</code> <p>If True, returns the (start, end) character indices of the tokens. If False, returns only the tokens.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[str]] | tuple[list[list[str]], Sequence[Sequence[OffsetType]]]</code> <p>The tokens of the input strings, and optionally the character spans of the tokens.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.__call__(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.__call__(with_offsets)","title":"<code>with_offsets</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(texts: str | list[str]) -&gt; list[list[str]]\n</code></pre> <p>Tokenizes one or more input texts.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The strings to tokenize.</p> required <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>A list of lists, each containing the tokens of the corresponding input string.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.detokenize","title":"detokenize  <code>abstractmethod</code>","text":"<pre><code>detokenize(\n    tokens: list[str] | list[list[str]],\n) -&gt; list[str]\n</code></pre> <p>Detokenizes the input tokens.</p> <p>Parameters:</p> Name Type Description Default <code>list[str] | list[list[str]]</code> <p>The tokens of one or more strings to detokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list containing the detokenized string(s).</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.detokenize(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize_with_offsets","title":"tokenize_with_offsets  <code>abstractmethod</code>","text":"<pre><code>tokenize_with_offsets(\n    texts: str | list[str],\n    add_gaps: bool = False,\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[list[list[str]], list[list[OffsetType]]]\n</code></pre> <p>Tokenizes the input texts and returns the character spans of the tokens.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The texts to tokenize.</p> required <code>bool</code> <p>Whether gaps should be added before/after tokens and offsets.</p> <code>False</code> <code>str</code> <p>The token to use for gaps. Default: <code>\u2581</code>.</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>The tokens of the input texts, and tuples <code>(start_idx, end_idx)</code> marking the position of tokens</p> <code>list[list[OffsetType]]</code> <p>in the original text. If the token is not present in the original text, None is used instead.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize_with_offsets(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize_with_offsets(add_gaps)","title":"<code>add_gaps</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.Tokenizer.tokenize_with_offsets(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer","title":"labl.utils.tokenizer.WhitespaceTokenizer","text":"<pre><code>WhitespaceTokenizer(word_delimiter: str = ' ')\n</code></pre> <p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer that uses whitespace to split the input strings into tokens.</p> <p>Hardcodes the <code>Compose([Strip(), ReduceToListOfListOfWords()])</code> transformation for tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The delimiter to use for splitting words. Defaults to whitespace.</p> <code>' '</code>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer(word_delimiter)","title":"<code>word_delimiter</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.detokenize","title":"detokenize","text":"<pre><code>detokenize(\n    tokens: list[str] | list[list[str]],\n) -&gt; list[str]\n</code></pre> <p>Detokenizes the input tokens using whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>list[str] | list[list[str]]</code> <p>The tokens of one or more strings to detokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list containing the detokenized string(s).</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.detokenize(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.tokenize_with_offsets","title":"tokenize_with_offsets","text":"<pre><code>tokenize_with_offsets(\n    texts: str | list[str],\n    add_gaps: bool = False,\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[list[list[str]], list[list[OffsetType]]]\n</code></pre> <p>Tokenizes the input texts and returns the character spans of the tokens.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The strings to tokenize.</p> required <code>bool</code> <p>Whether gaps should be added before/after tokens and offsets.</p> <code>False</code> <code>str</code> <p>The token to use for gaps. Default: <code>\u2581</code>.</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>The tokens of the input texts, and tuples (start_idx, end_idx) marking the position of tokens</p> <code>list[list[OffsetType]]</code> <p>in the original text. If the token is not present in the original text, None is used instead.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.tokenize_with_offsets(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.tokenize_with_offsets(add_gaps)","title":"<code>add_gaps</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WhitespaceTokenizer.tokenize_with_offsets(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer","title":"labl.utils.tokenizer.WordBoundaryTokenizer","text":"<pre><code>WordBoundaryTokenizer(exp: str = SPLIT_REGEX)\n</code></pre> <p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer that uses word boundaries to split the input strings into tokens.</p> <p>Hardcodes the <code>Compose([Strip(), RegexReduceToListOfListOfWords()])</code> transformation for tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The Regex expression to use for splitting. Defaults to <code>r\"[\\w']+|[.,!?:;'\u201d#$%&amp;\\(\\)\\*\\+-/&lt;=&gt;@\\[\\]^_{|}~\"]</code>. This regex keeps words (including contractions) together as single tokens, and treats each punctuation mark or special character as its own separate token.</p> <code>SPLIT_REGEX</code>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer(exp)","title":"<code>exp</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.detokenize","title":"detokenize","text":"<pre><code>detokenize(\n    tokens: list[str] | list[list[str]],\n) -&gt; list[str]\n</code></pre> <p>Detokenizes the input tokens using word boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>list[str] | list[list[str]]</code> <p>The tokens of one or more texts to detokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list containing the detokenized string(s).</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.detokenize(tokens)","title":"<code>tokens</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.tokenize_with_offsets","title":"tokenize_with_offsets","text":"<pre><code>tokenize_with_offsets(\n    texts: str | list[str],\n    add_gaps: bool = False,\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[list[list[str]], list[list[OffsetType]]]\n</code></pre> <p>Tokenizes the input texts and returns the character spans of the tokens.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The strings to tokenize.</p> required <code>bool</code> <p>Whether gaps should be added before/after tokens and offsets.</p> <code>False</code> <code>str</code> <p>The token to use for gaps. Default: <code>\u2581</code>.</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>The tokens of the input texts, and tuples (start_idx, end_idx) marking the position of tokens</p> <code>list[list[OffsetType]]</code> <p>in the original text. If the token is not present in the original text, None is used instead.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.tokenize_with_offsets(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.tokenize_with_offsets(add_gaps)","title":"<code>add_gaps</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.WordBoundaryTokenizer.tokenize_with_offsets(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer","title":"labl.utils.tokenizer.HuggingfaceTokenizer","text":"<pre><code>HuggingfaceTokenizer(\n    tokenizer_or_id: str\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast,\n    add_special_tokens: bool = False,\n    has_bos_token: bool = True,\n    has_eos_token: bool = True,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer that uses a <code>transformers.PreTrainedTokenizer</code> to split the input strings into tokens. Hardcodes the <code>ReduceToListOfListOfTokens</code> transformation for tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedTokenizer | PreTrainedTokenizerFast</code> <p>The tokenizer or its ID. If a string is provided, it will be used to load the tokenizer from the <code>transformers</code> library.</p> required <code>bool</code> <p>Whether to add special tokens to the tokenized output. Defaults to False.</p> <code>False</code> <code>bool</code> <p>Whether the tokenizer sets a beginning-of-sequence token. Defaults to True.</p> <code>True</code> <code>bool</code> <p>Whether the tokenizer sets an end-of-sequence token. Defaults to True.</p> <code>True</code> <code>dict</code> <p>Additional keyword arguments to pass to the tokenizer initialization.</p> <code>{}</code>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer(tokenizer_or_id)","title":"<code>tokenizer_or_id</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer(add_special_tokens)","title":"<code>add_special_tokens</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer(has_bos_token)","title":"<code>has_bos_token</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer(has_eos_token)","title":"<code>has_eos_token</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer(kwargs)","title":"<code>kwargs</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer.detokenize","title":"detokenize","text":"<pre><code>detokenize(\n    tokens: list[str] | list[list[str]],\n) -&gt; list[str]\n</code></pre>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer.tokenize_with_offsets","title":"tokenize_with_offsets","text":"<pre><code>tokenize_with_offsets(\n    texts: str | list[str],\n    add_gaps: bool = False,\n    gap_token: str = \"\u2581\",\n) -&gt; tuple[list[list[str]], list[list[OffsetType]]]\n</code></pre> <p>Tokenizes the input texts and returns the character spans of the tokens.</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str]</code> <p>The strings to tokenize.</p> required <code>bool</code> <p>Whether gaps should be added before/after tokens and offsets.</p> <code>False</code> <code>str</code> <p>The token to use for gaps. Default: <code>\u2581</code>.</p> <code>'\u2581'</code> <p>Returns:</p> Type Description <code>tuple[list[list[str]], list[list[OffsetType]]]</code> <p>The tokens of the input texts, and the character spans of the tokens.</p>"},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer.tokenize_with_offsets(texts)","title":"<code>texts</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer.tokenize_with_offsets(add_gaps)","title":"<code>add_gaps</code>","text":""},{"location":"api/utils/tokenizer/#labl.utils.tokenizer.HuggingfaceTokenizer.tokenize_with_offsets(gap_token)","title":"<code>gap_token</code>","text":""},{"location":"api/utils/utils/","title":"Utility Classes","text":""},{"location":"api/utils/utils/#spans","title":"\ud83d\udd24 Spans","text":""},{"location":"api/utils/utils/#labl.utils.span.Span","title":"labl.utils.span.Span  <code>dataclass</code>","text":"<pre><code>Span(\n    start: int,\n    end: int,\n    label: LabelType,\n    text: str | None = None,\n)\n</code></pre> <p>Class representing a span in a text.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>The starting index of the span.</p> <code>end</code> <code>int</code> <p>The ending index of the span.</p> <code>label</code> <code>str | int | float | None</code> <p>The label of the span.</p> <code>text</code> <code>str | None</code> <p>The text of the span. Defaults to None.</p>"},{"location":"api/utils/utils/#labl.utils.span.Span.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict) -&gt; Span\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.span.Span.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(data: Sequence[Span | dict]) -&gt; SpanList\n</code></pre> <p>Creates a list of span instances from a sequence of spans and/or primitive types.</p> <p>Parameters:</p> Name Type Description Default <code>list</code> <p>List of span instances or dictionaries from which they can be initialized.</p> required <p>Returns:</p> Type Description <code>SpanList</code> <p>A list of span instances.</p>"},{"location":"api/utils/utils/#labl.utils.span.Span.from_list(data)","title":"<code>data</code>","text":""},{"location":"api/utils/utils/#labl.utils.span.Span.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; SpanType\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.span.SpanList","title":"labl.utils.span.SpanList","text":"<p>               Bases: <code>list[Span]</code></p> <p>Class for a list of <code>Span</code>, with custom visualization.</p>"},{"location":"api/utils/utils/#tokens","title":"\ud83d\udd20 Tokens","text":""},{"location":"api/utils/utils/#labl.utils.token.LabeledToken","title":"labl.utils.token.LabeledToken  <code>dataclass</code>","text":"<pre><code>LabeledToken(token: str, label: LabelType)\n</code></pre> <p>Class for a token with an associated label.</p> <p>Attributes:</p> Name Type Description <code>token</code> <code>str</code> <p>The token. Can be accessed using <code>.t</code>.</p> <code>label</code> <code>str | int | float | None</code> <p>The label associated with the token. Can be accessed using <code>.l</code>.</p>"},{"location":"api/utils/utils/#labl.utils.token.LabeledToken.from_tuple","title":"from_tuple  <code>classmethod</code>","text":"<pre><code>from_tuple(tup: tuple[str, LabelType]) -&gt; LabeledToken\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.token.LabeledToken.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(\n    lst: LabeledTokenInput,\n    keep_labels: Sequence[LabelType] = [],\n    ignore_labels: Sequence[LabelType] = [],\n) -&gt; LabeledTokenList\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.token.LabeledToken.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple()\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.token.LabeledTokenList","title":"labl.utils.token.LabeledTokenList","text":"<p>               Bases: <code>list[LabeledToken]</code></p> <p>Class for a list of <code>LabeledToken</code>, with custom visualization.</p>"},{"location":"api/utils/utils/#transforms","title":"\ud83d\udd04 Transforms","text":""},{"location":"api/utils/utils/#labl.utils.transform.RegexReduceToListOfListOfWords","title":"labl.utils.transform.RegexReduceToListOfListOfWords","text":"<pre><code>RegexReduceToListOfListOfWords(exp: str = SPLIT_REGEX)\n</code></pre> <p>               Bases: <code>ReduceToListOfListOfWords</code></p> <p>Version of <code>ReduceToListOfWords</code> using Regex for splitting.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The Regex expression to use for splitting. Defaults to <code>r\"[\\w']+|[.,!?:;'\u201d#$%&amp;\\(\\)\\*\\+-/&lt;=&gt;@\\[\\]^_{|}~\"]</code>. This regex keeps words (including contractions) together as single tokens, and treats each punctuation mark or special character as its own separate token.</p> <code>SPLIT_REGEX</code> Source code in <code>labl/utils/transform.py</code> <pre><code>def __init__(self, exp: str = SPLIT_REGEX):\n    \"\"\"\n    Args:\n        exp: the Regex expression to use for splitting.\"\n    \"\"\"\n    self.exp = exp\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.transform.RegexReduceToListOfListOfWords(exp)","title":"<code>exp</code>","text":""},{"location":"api/utils/utils/#labl.utils.transform.ReduceToListOfListOfTokens","title":"labl.utils.transform.ReduceToListOfListOfTokens","text":"<pre><code>ReduceToListOfListOfTokens(\n    tokenizer_or_id: str\n    | PreTrainedTokenizer\n    | PreTrainedTokenizerFast,\n    add_special_tokens: bool = False,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>ReduceToListOfListOfWords</code></p> <p>Version of <code>ReduceToListOfWords</code> using a tokenizer from the <code>transformers</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedTokenizer | PreTrainedTokenizerFast</code> <p>The tokenizer or its ID. If a string is provided, it will be used to load the tokenizer from the <code>transformers</code> library.</p> required <code>bool</code> <p>Whether to add special tokens to the tokenized output. Defaults to False.</p> <code>False</code> Source code in <code>labl/utils/transform.py</code> <pre><code>def __init__(\n    self,\n    tokenizer_or_id: str | PreTrainedTokenizer | PreTrainedTokenizerFast,\n    add_special_tokens: bool = False,\n    **kwargs,\n):\n    self.add_special_tokens = add_special_tokens\n    if isinstance(tokenizer_or_id, str):\n        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\n            tokenizer_or_id, use_fast=True, **kwargs\n        )\n    else:\n        if kwargs:\n            logger.warning(f\"Ignoring additional keyword arguments for tokenizer initialization: {kwargs}.\")\n        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = tokenizer_or_id\n</code></pre>"},{"location":"api/utils/utils/#labl.utils.transform.ReduceToListOfListOfTokens(tokenizer_or_id)","title":"<code>tokenizer_or_id</code>","text":""},{"location":"api/utils/utils/#labl.utils.transform.ReduceToListOfListOfTokens(add_special_tokens)","title":"<code>add_special_tokens</code>","text":""},{"location":"api/utils/utils/#aggregation-functions","title":"\ud83e\udd1d Aggregation Functions","text":""},{"location":"api/utils/utils/#labl.utils.aggregation.LabelAggregation","title":"labl.utils.aggregation.LabelAggregation","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for label aggregation functions.</p>"},{"location":"api/utils/utils/#labl.utils.aggregation.LabelAggregation.__call__","title":"__call__","text":"<pre><code>__call__(labels: Sequence[LabelType]) -&gt; Any\n</code></pre> <p>Aggregate the labels.</p> <p>Parameters:</p> Name Type Description Default <code>Sequence[Any]</code> <p>The labels to aggregate.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The aggregated label.</p>"},{"location":"api/utils/utils/#labl.utils.aggregation.LabelAggregation.__call__(labels)","title":"<code>labels</code>","text":""},{"location":"api/utils/utils/#labl.utils.aggregation.label_sum_aggregation","title":"labl.utils.aggregation.label_sum_aggregation","text":"<pre><code>label_sum_aggregation(labels: Sequence[T]) -&gt; T | None\n</code></pre> Source code in <code>labl/utils/aggregation.py</code> <pre><code>def label_sum_aggregation(labels: Sequence[T]) -&gt; T | None:\n    if not labels:\n        return None\n    out = labels[0]\n    for l in labels[1:]:\n        if type(l) is not type(out):\n            raise RuntimeError(f\"Different types found during aggregation: {type(l)} and {type(out)}\")\n        out += l  # type: ignore\n    return out\n</code></pre>"},{"location":"api/utils/utils/#labeledinterface","title":"\ud83c\udff7\ufe0f LabeledInterface","text":""},{"location":"api/utils/utils/#labl.data.labeled_interface.LabeledInterface","title":"labl.data.labeled_interface.LabeledInterface","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/utils/utils/#labl.data.labeled_interface.LabeledInterface.label_types","title":"label_types  <code>property</code> <code>writable</code>","text":"<pre><code>label_types: list[type]\n</code></pre> <p>Returns the list of label types for the entry. This is a read-only property.</p>"},{"location":"api/utils/utils/#labl.data.labeled_interface.LabeledInterface.relabel","title":"relabel  <code>abstractmethod</code>","text":"<pre><code>relabel(\n    relabel_fn: Callable[[LabelType], LabelType]\n    | None = None,\n    relabel_map: dict[str | int, LabelType] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>labl/data/labeled_interface.py</code> <pre><code>@abstractmethod\ndef relabel(\n    self,\n    relabel_fn: Callable[[LabelType], LabelType] | None = None,\n    relabel_map: dict[str | int, LabelType] | None = None,\n) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/utils/utils/#labl.data.labeled_interface.LabeledInterface.to_dict","title":"to_dict  <code>abstractmethod</code>","text":"<pre><code>to_dict() -&gt; SerializableDictType\n</code></pre> Source code in <code>labl/data/labeled_interface.py</code> <pre><code>@abstractmethod\ndef to_dict(self) -&gt; SerializableDictType:\n    pass\n</code></pre>"},{"location":"api/utils/utils/#labl.data.labeled_interface.LabeledInterface.from_dict","title":"from_dict  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>from_dict(\n    data: SerializedLabeledObject,\n) -&gt; LabeledInterface\n</code></pre> <p>Creates an instance of the class from a dictionary representation.</p> Source code in <code>labl/data/labeled_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_dict(cls, data: SerializedLabeledObject) -&gt; \"LabeledInterface\":  # type: ignore\n    \"\"\"Creates an instance of the class from a dictionary representation.\"\"\"\n    pass\n</code></pre>"},{"location":"notebooks/edit_analysis/","title":"Edit Analysis","text":"<pre><code># type: ignore\nfrom datasets import load_dataset\n\nfull_main_dict = load_dataset(\"gsarti/qe4pe\", \"main\")\nfull_main = full_main_dict[\"train\"].to_pandas()\nmain = full_main[(~full_main[\"has_issue\"]) &amp;amp; (full_main[\"translator_main_id\"] != \"no_highlight_t4\")]\n\nita_main = main[main[\"tgt_lang\"] == \"ita\"].reset_index(drop=True)\nnld_main = main[main[\"tgt_lang\"] == \"nld\"].reset_index(drop=True)\n\nprint(\"Italian main data:\", len(ita_main), \"total edits\")\nprint(\"Dutch main data:\", len(nld_main), \"total edits\")\n</code></pre> <pre>\n<code>Italian main data: 3780 total edits\nDutch main data: 3780 total edits\n</code>\n</pre> <p>We will now create an <code>EditDataset</code> containing the multiple post-edits for each sentence using the <code>from_edits_dataframe</code> method, allowing for quick import from a <code>pandas</code> DataFrame. The required columns are:</p> <ul> <li><code>text_column</code>: The name of the column containing the text before edits.</li> <li><code>edit_column</code>: The name of the column containing the text after edits.</li> <li><code>entry_ids</code>: A list of column names to be used as unique identifiers for each entry. This is useful when the same sentence has multiple edits, as in this case.</li> </ul> <pre><code>from labl import EditedDataset\n\nita = EditedDataset.from_edits_dataframe(\n    ita_main,\n    text_column=\"mt_text\",\n    edit_column=\"pe_text\",\n    entry_ids=[\"doc_id\", \"segment_in_doc_id\"],\n)\nprint(\"Italian main data:\", len(ita), \"unique entries\")\n\nnld = EditedDataset.from_edits_dataframe(\n    nld_main,\n    text_column=\"mt_text\",\n    edit_column=\"pe_text\",\n    entry_ids=[\"doc_id\", \"segment_in_doc_id\"],\n)\nprint(\"Dutch main data:\", len(nld), \"unique entries\")\n</code></pre> <pre>\n<code>None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\nExtracting texts and edits: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [00:00&lt;00:00, 1501.24entries/s]\nCreating EditedDataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [00:00&lt;00:00, 777.58entries/s]\n</code>\n</pre> <pre>\n<code>Italian main data: 315 unique entries\n</code>\n</pre> <pre>\n<code>Extracting texts and edits: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [00:00&lt;00:00, 1516.79entries/s]\nCreating EditedDataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [00:00&lt;00:00, 861.47entries/s]</code>\n</pre> <pre>\n<code>Dutch main data: 315 unique entries\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <p>We can now visualize the contents of each entry by simply printing it. <code>EditedDataset</code> is a list-like object containing entries, and since multiple edits are available for each entry, every entry is also a list-like object of <code>EditedEntry</code>. An <code>EditedEntry</code> is, in essence, a combination of two <code>LabeledEntry</code> objects (see the Quickstart tutorial), one for the original text and one for the edited text, plus some additional information regarding edit alignments.</p> <pre><code># Accessing all edits for the first unique entry\nid_0_all_edits = ita[0]\n\n# Accessing the first edit for the fist unique entry\nid_0_first_edit = ita[0][5]\n\n# Visualize the contents of an edited entry\nprint(id_0_first_edit)\n</code></pre> <pre>\n<code>orig.text:\n            Esistono limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\nedit.text:\n            Esistono ricerche limitate riguardanti la costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\norig.tokens:\n            \u2581 Esistono \u2581 limitate \u2581 ricerche \u2581 riguardanti \u2581 la \u2581 continuit\u00e0, \u2581 la \u2581 stabilit\u00e0 \u2581 e \u2581 il \u2581 ruolo \u2581 del \u2581 paese \u2581 di \u2581 origine \u2581 nel \u2581 temperamento \u2581 del \u2581 neonato \u2581 prematuro \u2581 durante \u2581 il \u2581 primo \u2581 anno \u2581 di \u2581 vita. \u2581\n                       I                   D                                S                                                                                                                                                             \n\nedit.tokens:\n            \u2581 Esistono \u2581 ricerche \u2581 limitate \u2581 riguardanti \u2581 la \u2581 costanza, \u2581 la \u2581 stabilit\u00e0 \u2581 e \u2581 il \u2581 ruolo \u2581 del \u2581 paese \u2581 di \u2581 origine \u2581 nel \u2581 temperamento \u2581 del \u2581 neonato \u2581 prematuro \u2581 durante \u2581 il \u2581 primo \u2581 anno \u2581 di \u2581 vita. \u2581\n                                I            D                            S                                                                                                                                                             \n\naligned:\n            ORIG: Esistono ******** limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n            EDIT: Esistono ricerche limitate ******** riguardanti la   costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                                  I                 D                          S\n</code>\n</pre> <p>The <code>aligned</code> attribute is obtained using <code>jiwer</code> and corresponds to the Levenshtein alignment between the original and edited text. Since no tokenizer was provided, whitespace tokenization was used by default.</p> <pre><code># Merge gap annotations in-place\nita.merge_gap_annotations(keep_final_gap=False)\nprint(ita[0][5])\n</code></pre> <pre>\n<code>orig.text:\n            Esistono limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\nedit.text:\n            Esistono ricerche limitate riguardanti la costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\norig.tokens:\n            Esistono limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                            I        D                          S                                                                                                                   \n\nedit.tokens:\n            Esistono ricerche limitate riguardanti la costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                            I                    D            S                                                                                                                   \n\naligned:\n            ORIG: Esistono ******** limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n            EDIT: Esistono ricerche limitate ******** riguardanti la   costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                                  I                 D                          S\n</code>\n</pre> <pre><code>agreement_output = ita.get_agreement()\nprint(agreement_output)\n</code></pre> <pre>\n<code>AgreementOutput(\n    type: krippendorff_nominal,\n    full: 0.3234,\n    pair:\n            | A0   | A1   | A2   | A3   | A4   | A5   | A6   | A7   | A8   | A9   | A10  | A11  |\n        A0  |      | 0.36 | 0.53 | 0.32 | 0.18 | 0.37 | 0.35 | 0.36 | 0.41 | 0.35 | 0.35 | 0.37 |\n        A1  | 0.36 |      | 0.18 | 0.32 | 0.33 | 0.27 | 0.4  | 0.42 | 0.35 | 0.32 | 0.36 | 0.34 |\n        A2  | 0.53 | 0.18 |      | 0.45 | 0.23 | 0.37 | 0.39 | 0.35 | 0.34 | 0.56 | 0.34 | 0.38 |\n        A3  | 0.32 | 0.32 | 0.45 |      | 0.34 | 0.38 | 0.34 | 0.32 | 0.33 | 0.38 | 0.29 | 0.41 |\n        A4  | 0.18 | 0.33 | 0.23 | 0.34 |      | 0.32 | 0.33 | 0.31 | 0.28 | 0.21 | 0.33 | 0.27 |\n        A5  | 0.37 | 0.27 | 0.37 | 0.38 | 0.32 |      | 0.3  | 0.34 | 0.31 | 0.33 | 0.34 | 0.32 |\n        A6  | 0.35 | 0.4  | 0.39 | 0.34 | 0.33 | 0.3  |      | 0.31 | 0.28 | 0.27 | 0.3  | 0.3  |\n        A7  | 0.36 | 0.42 | 0.35 | 0.32 | 0.31 | 0.34 | 0.31 |      | 0.34 | 0.4  | 0.34 | 0.34 |\n        A8  | 0.41 | 0.35 | 0.34 | 0.33 | 0.28 | 0.31 | 0.28 | 0.34 |      | 0.31 | 0.33 | 0.35 |\n        A9  | 0.35 | 0.32 | 0.56 | 0.38 | 0.21 | 0.33 | 0.27 | 0.4  | 0.31 |      | 0.36 | 0.3  |\n        A10 | 0.35 | 0.36 | 0.34 | 0.29 | 0.33 | 0.34 | 0.3  | 0.34 | 0.33 | 0.36 |      | 0.35 |\n        A11 | 0.37 | 0.34 | 0.38 | 0.41 | 0.27 | 0.32 | 0.3  | 0.34 | 0.35 | 0.3  | 0.35 |      |\n\n)\n\n</code>\n</pre> <p>The agreement is quite low, but currently we are considering every type of edit as a separate label (including the combinations derived from merging, e.g. <code>IS</code> and <code>ID</code>). We can try to relabel the entries to use a single label to mark edits (e.g. <code>E</code>), and see how this affects the agreement computation. Relabeling with the <code>relabel</code> method can be done either with a <code>relabel_map</code> dictionary specifying the mapping from old to new labels, or with a <code>relabel_fn</code> function that takes a label and returns the new label. The latter is useful when we want to apply a more complex relabeling strategy, such as merging multiple labels into one.</p> <p>\u26a0\ufe0f While relabeling affects all properties of the <code>orig</code> and <code>edit</code> <code>LabeledEntry</code> attributes in each <code>EditedEntry</code>, it does not affect the <code>aligned</code> attribute, which cannot be changed after the entry is created. This does not affect in any way the rest of the analysis.</p> <pre><code>ita.relabel(lambda lab: \"E\" if lab is not None else None)\n\n# Visualize the contents of an edited entry\nprint(ita[0][5])\n</code></pre> <pre>\n<code>orig.text:\n            Esistono limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\nedit.text:\n            Esistono ricerche limitate riguardanti la costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\norig.tokens:\n            Esistono limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                            E        E                          E                                                                                                                   \n\nedit.tokens:\n            Esistono ricerche limitate riguardanti la costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                            E                    E            E                                                                                                                   \n\naligned:\n            ORIG: Esistono ******** limitate ricerche riguardanti la continuit\u00e0, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n            EDIT: Esistono ricerche limitate ******** riguardanti la   costanza, la stabilit\u00e0 e il ruolo del paese di origine nel temperamento del neonato prematuro durante il primo anno di vita.\n                                  I                 D                          S\n</code>\n</pre> <pre><code>agreement_output = ita.get_agreement()\nprint(agreement_output)\n</code></pre> <pre>\n<code>AgreementOutput(\n    type: spearmanr_binary,\n    full: None,\n    pair:\n            | A0   | A1   | A2   | A3   | A4   | A5   | A6   | A7   | A8   | A9   | A10  | A11  |\n        A0  |      | 0.34 | 0.22 | 0.33 | 0.27 | 0.3  | 0.33 | 0.27 | 0.28 | 0.3  | 0.27 | 0.27 |\n        A1  | 0.34 |      | 0.22 | 0.38 | 0.33 | 0.36 | 0.4  | 0.32 | 0.34 | 0.34 | 0.36 | 0.35 |\n        A2  | 0.22 | 0.22 |      | 0.26 | 0.21 | 0.23 | 0.25 | 0.18 | 0.22 | 0.28 | 0.2  | 0.21 |\n        A3  | 0.33 | 0.38 | 0.26 |      | 0.36 | 0.37 | 0.39 | 0.31 | 0.36 | 0.36 | 0.35 | 0.35 |\n        A4  | 0.27 | 0.33 | 0.21 | 0.36 |      | 0.4  | 0.35 | 0.32 | 0.39 | 0.25 | 0.34 | 0.34 |\n        A5  | 0.3  | 0.36 | 0.23 | 0.37 | 0.4  |      | 0.37 | 0.34 | 0.37 | 0.33 | 0.34 | 0.38 |\n        A6  | 0.33 | 0.4  | 0.25 | 0.39 | 0.35 | 0.37 |      | 0.37 | 0.37 | 0.35 | 0.38 | 0.38 |\n        A7  | 0.27 | 0.32 | 0.18 | 0.31 | 0.32 | 0.34 | 0.37 |      | 0.37 | 0.34 | 0.39 | 0.4  |\n        A8  | 0.28 | 0.34 | 0.22 | 0.36 | 0.39 | 0.37 | 0.37 | 0.37 |      | 0.3  | 0.36 | 0.37 |\n        A9  | 0.3  | 0.34 | 0.28 | 0.36 | 0.25 | 0.33 | 0.35 | 0.34 | 0.3  |      | 0.33 | 0.31 |\n        A10 | 0.27 | 0.36 | 0.2  | 0.35 | 0.34 | 0.34 | 0.38 | 0.39 | 0.36 | 0.33 |      | 0.4  |\n        A11 | 0.27 | 0.35 | 0.21 | 0.35 | 0.34 | 0.38 | 0.38 | 0.4  | 0.37 | 0.31 | 0.4  |      |\n\n)\n\n</code>\n</pre> <p>The new agreement is now a Spearman's rank correlation coefficient, since the relabeling resulted in a binary labeling scheme. We can mark all unchanged tokens with a label <code>K</code> for \"kept\" to compute the agreement on both <code>E</code> and <code>K</code> labels. Correlation is not defined across multiple label sets, so the <code>full</code> attribute is <code>None</code>.</p> <pre><code>from labl.data import LabeledDataset\n\nita_main_unique = ita_main.groupby([\"doc_id\", \"segment_in_doc_id\"]).first().reset_index(drop=True)\n\nall_spans = []\nfor spans_str in ita_main_unique[\"mt_xcomet_errors\"]:\n    curr_spans = []\n    list_dic_span = eval(spans_str)\n    for span in list_dic_span:\n        curr_spans.append(\n            {\n                \"start\": span[\"start\"],\n                \"end\": span[\"end\"],\n                \"label\": span[\"severity\"],\n                \"text\": span[\"text\"],\n            }\n        )\n    all_spans.append(curr_spans)\n\nita_xcomet_spans = LabeledDataset.from_spans(\n    texts=list(ita_main_unique[\"mt_text\"]),\n    spans=all_spans,\n)\n</code></pre> <pre>\n<code>Creating labeled dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 315/315 [00:00&lt;00:00, 2770.52entries/s]\n</code>\n</pre> <pre><code>print(ita_xcomet_spans[5])\n</code></pre> <pre>\n<code>text:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso e della risata e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\ntagged:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso&lt;minor&gt; e&lt;/minor&gt; della&lt;minor&gt; risata&lt;/minor&gt; e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\ntokens:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso     e della risata e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\n                                                                                                                                         minor        minor                                                                                                                                                                             \n\nspans:\n       0: 129:131 (e) =&gt; minor\n       1: 137:144 (risata) =&gt; minor\n</code>\n</pre> <pre><code>ita_xcomet_spans.relabel(lambda lab: \"E\" if lab is not None else None)\nprint(ita_xcomet_spans[5])\n</code></pre> <pre>\n<code>text:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso e della risata e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\ntagged:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso&lt;E&gt; e&lt;/E&gt; della&lt;E&gt; risata&lt;/E&gt; e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\ntokens:\n       La continuit\u00e0 del temperamento dai 6 ai 12 mesi varia a seconda del paese: le madri cilene hanno riportato un aumento del sorriso e della risata e del livello di attivit\u00e0 dai 6 ai 12 mesi, e le madri del Regno Unito hanno riportato una diminuzione del sorriso e della risata e un aumento della paura dai 6 ai 12 mesi.\n                                                                                                                                         E            E                                                                                                                                                                             \n\nspans:\n       0: 129:131 (None) =&gt; E\n       1: 137:144 (None) =&gt; E\n</code>\n</pre> <pre><code>for idx in range(len(ita[0])):\n    agreement = ita_xcomet_spans.get_agreement(LabeledDataset([e[idx].orig for e in ita]))\n    print(f\"Agreement of XCOMET with annotator {idx}: {agreement.pair}\")\n</code></pre> <pre>\n<code>Agreement of XCOMET with annotator 0: 0.21915394517009773\nAgreement of XCOMET with annotator 1: 0.2275337204552564\nAgreement of XCOMET with annotator 2: 0.22868301380157635\nAgreement of XCOMET with annotator 3: 0.20886058547534597\nAgreement of XCOMET with annotator 4: 0.18324181750361304\nAgreement of XCOMET with annotator 5: 0.2350649104677996\nAgreement of XCOMET with annotator 6: 0.2599132539885884\nAgreement of XCOMET with annotator 7: 0.1887801438815674\nAgreement of XCOMET with annotator 8: 0.18871477020233016\nAgreement of XCOMET with annotator 9: 0.24598840796027605\nAgreement of XCOMET with annotator 10: 0.1912293643198062\nAgreement of XCOMET with annotator 11: 0.19964725018467855\n</code>\n</pre>"},{"location":"notebooks/edit_analysis/#word-level-edit-analysis-with-labl","title":"Word-level Edit Analysis with <code>labl</code> \ud83c\udff7\ufe0f","text":"<p>In this notebook, we will use <code>labl</code> to analyze machine translation post-edits from multiple annotators, extracting useful statistics and visualizations. Finally, we will compare the annotator edit proportions with the error spans predicted by the word-level quality estimation model <code>XCOMET-XXL</code> to evaluate its performance.</p> <p>Firstly, we load some edit data hosted on the \ud83e\udd17 <code>datasets</code> Hub. For this purpose, we will use the QE4PE dataset, containing a set of 315 sentences each with 12 human post-edits for English-Italian and English-Dutch (more info). The large amount of annotators will prove useful for analyzing agreement.</p>"},{"location":"notebooks/edit_analysis/#handling-gaps","title":"Handling gaps","text":"<p>You might also note that <code>orig.tokens</code> and <code>edit.tokens</code> contain gap tokens (<code>\u2581</code>, see e.g. the MLQE-PE dataset for an example of gap usage). These are added by default when importing edits to keep annotations for insertions and deletions distinct on both sequences (for example, the insertion label <code>I</code> on the second gap of <code>orig.tokens</code> marks that the token <code>ricerche</code> was added in that position in <code>edit.tokens</code>, while the deletion label <code>D</code> on the fourth gap of <code>edit.tokens</code> marks that the token <code>ricerche</code> was deleted from <code>orig.tokens</code>). </p> <p>If you want to restrict analysis on the actual tokens, gap annotations can be trasferred to the token on the right to obtain a more compact representation of the sequence. By default, labels are added together (so if a gap marked with <code>I</code> is followed by a token marked with <code>S</code>, the resulting label will be <code>IS</code>), but the merging behavior can be customized with the <code>merge_fn</code> argument:</p>"},{"location":"notebooks/edit_analysis/#agreement","title":"Agreement","text":"<p>We can now easily obtain a measure of the edit agreement between annotators using <code>get_agreement</code> using Krippendorff's alpha coefficient. Provided that every entry has multiple edits, the agreement will be computed across all annotations of the original text, and for every annotator pair:</p>"},{"location":"notebooks/quickstart/","title":"Quickstart","text":""},{"location":"notebooks/quickstart/#labl-quickstart","title":"<code>labl</code> \ud83c\udff7\ufe0f Quickstart","text":""}]}